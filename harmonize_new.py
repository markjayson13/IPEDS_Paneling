"""Label-driven IPEDS harmonizer.

This module implements the replacement for ``harmonize.py`` using a "Power User"
architecture. It consumes the concept catalog defined in ``concept_catalog.py``,
the dictionary lake produced by ``01_ingest_dictionaries.py``, and the manifest
files generated by ``download_ipeds.py``. Concepts are resolved by matching
normalized dictionary labels, data files are located through the per-year
manifest (with fallbacks), and the resulting long-form panel is validated using
``validation_rules.yaml``.

Starting in the 2024-25 collection, Cost of Attendance (COA) and net price
labels may surface under CST forms rather than SFA; the harmonizer honors that
shift through the concept catalog's form mappings.

Example
-------
To harmonize 2017-2018 data after running the downloader and dictionary ingest::

    python harmonize_new.py --root /path/to/raw --lake dictionary_lake.parquet \
        --years 2017:2018 --output panel_long.parquet --strict-release

The script will emit ``panel_long.parquet`` with tidy concept values,
``label_matches.csv`` with the label resolution audit, and
``validation_report.csv`` summarizing post-extraction checks.
"""

from __future__ import annotations

import argparse
import logging
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence

try:  # pylint: disable=wrong-import-position
    import pandas as pd
except ImportError as exc:  # pragma: no cover - startup guard
    sys.stderr.write(
        "pandas/openpyxl/xlrd missing. Run: source .venv/bin/activate && pip install -r requirements.txt\n"
    )
    raise
import yaml

from concept_catalog import CONCEPTS, GLOBAL_EXCLUDE

MIN_ACCEPT_SCORE = 3.5
if MIN_ACCEPT_SCORE < 3.5:  # pragma: no cover - sanity guard
    raise ValueError("MIN_ACCEPT_SCORE must be at least 3.5")
TOPK_AUDIT = 5
FINANCE_STOCK_EXCLUDE = re.compile(r"endowment market value|net assets", re.IGNORECASE)
IMPUTE_FLAG_PATTERN = re.compile(r"(imputation|impute|imputed|status\s*flag)", re.IGNORECASE)

METADATA_NEGATIVES = re.compile(
    r"^(unitid(_p)?|unit id|instnm|institution name|year|state|zip|fips|sector)$",
    re.IGNORECASE,
)

NA_TOKENS = [
    "PrivacySuppressed",
    "NULL",
    "NaN",
    "N/A",
    "D",
    "DP",
    "—",
    ".",
]

# Default output locations
PARQUET_OUTPUT_DIR = Path("/Users/markjaysonfarol13/Higher Ed research/IPEDS/Parquets")
CHECKS_OUTPUT_DIR = Path("/Users/markjaysonfarol13/Higher Ed research/IPEDS/Checks")
LABEL_CHECK_DIR = CHECKS_OUTPUT_DIR / "Label match"
LABEL_MATCH_PATH = LABEL_CHECK_DIR / "label_matches.csv"
VALIDATION_REPORT_PATH = CHECKS_OUTPUT_DIR / "validation_report.csv"
SUPP_PANEL_DIR = CHECKS_OUTPUT_DIR / "Supp. Panels"
FORM_CONFLICTS_PATH = CHECKS_OUTPUT_DIR / "form_conflicts.csv"
COVERAGE_SUMMARY_PATH = CHECKS_OUTPUT_DIR / "coverage_summary.csv"

OUTPUT_COLUMNS = [
    "UNITID",
    "reporting_unitid",
    "year",
    "target_var",
    "value",
    "concept",
    "units",
    "survey",
    "form_family",
    "finance_basis",
    "decision_score",
    "period_type",
    "source_var",
    "label_matched",
    "imputed_flag",
    "is_imputed",
    "source_file",
    "release",
    "notes",
    "reporting_map_policy",
    "reporting_unit_scope",
    "allocation_factor_used",
]

# Optional columns (e.g., 'state' for long-family extractions) are appended dynamically in build_output_frame.

_US_STATES_AND_JURIS = [
    "alabama",
    "alaska",
    "arizona",
    "arkansas",
    "california",
    "colorado",
    "connecticut",
    "delaware",
    "district of columbia",
    "florida",
    "georgia",
    "hawaii",
    "idaho",
    "illinois",
    "indiana",
    "iowa",
    "kansas",
    "kentucky",
    "louisiana",
    "maine",
    "maryland",
    "massachusetts",
    "michigan",
    "minnesota",
    "mississippi",
    "missouri",
    "montana",
    "nebraska",
    "nevada",
    "new hampshire",
    "new jersey",
    "new mexico",
    "new york",
    "north carolina",
    "north dakota",
    "ohio",
    "oklahoma",
    "oregon",
    "pennsylvania",
    "rhode island",
    "south carolina",
    "south dakota",
    "tennessee",
    "texas",
    "utah",
    "vermont",
    "virginia",
    "washington",
    "west virginia",
    "wisconsin",
    "wyoming",
    "puerto rico",
    "guam",
    "american samoa",
    "northern mariana islands",
    "u.s. virgin islands",
]

def _extract_state_token(label_norm: str) -> Optional[str]:
    if not label_norm:
        return None
    for state in _US_STATES_AND_JURIS:
        if state in label_norm:
            return state
    return None

STATIC_LOCATIONAL_TARGETS = {
    "dir_county_fips",
    "dir_county_name",
    "dir_congress_district",
    "dir_cbsa_code",
    "dir_cbsa_type",
    "dir_csa_code",
    "dir_necta_code",
    "dir_latitude",
    "dir_longitude",
}

CONCEPT_BY_TARGET_VAR = {
    str(concept.get("target_var")): concept for concept in CONCEPTS.values() if concept.get("target_var")
}


def _to_lower(text: Optional[str]) -> str:
    return (text or "").strip().lower()


def report_duplicate_modules() -> None:
    repo_root = Path(__file__).resolve().parent
    targets = {
        "01_ingest_dictionaries.py": repo_root / "01_ingest_dictionaries.py",
        "harmonize_new.py": Path(__file__).resolve(),
        "concept_catalog.py": repo_root / "concept_catalog.py",
    }
    for name, canonical in targets.items():
        canonical_path = canonical.resolve()
        matches = [p.resolve() for p in repo_root.rglob(name)]
        duplicates = [
            p
            for p in matches
            if p != canonical_path and ".venv" not in p.parts and "__pycache__" not in p.parts
        ]
        for dup in duplicates:
            print(f"REMOVE_AFTER_REVIEW duplicate module found: {dup}")


@dataclass
class CandidateSelection:
    concept_key: str
    year: int
    target_var: str
    score: Optional[float]
    n_candidates: int
    source_var: Optional[str]
    label_matched: Optional[str]
    dict_file: Optional[str]
    dict_filename: Optional[str]
    prefix_hint: Optional[str]
    survey_hint: Optional[str]
    chosen_data_file: Optional[str]
    manifest_release: Optional[str]
    notes: Optional[str]
    top_alternates: Optional[str]
    imputed_flag: Optional[str]
    accepted: Optional[bool] = None


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Label-driven IPEDS harmonizer")
    parser.add_argument("--root", type=Path, default=Path("data/raw"), help="Raw data root containing year folders")
    parser.add_argument("--lake", type=Path, default=Path("dictionary_lake.parquet"), help="Dictionary lake parquet path")
    parser.add_argument("--output", type=Path, default=PARQUET_OUTPUT_DIR / "panel_long.parquet", help="Output parquet path")
    parser.add_argument(
        "--years",
        type=str,
        default="2004:2024",
        help="Year expression: 'YYYY', 'YYYY:YYYY', or comma list 'YYYY,YYYY'",
    )
    parser.add_argument("--rules", type=Path, default=Path("validation_rules.yaml"), help="Validation rules YAML path")
    parser.add_argument("--strict-release", action="store_true", help="Error if mixed provisional/revised releases")
    parser.add_argument("--strict-coverage", action="store_true", help="Error if any concept fails to meet MIN_ACCEPT_SCORE")
    parser.add_argument("--log-level", type=str, default="INFO", help="Logging level (e.g., INFO, DEBUG)")
    parser.add_argument("--reporting-map", type=Path, default=None, help="CSV crosswalk with UNITID->reporting_unitid actions")
    parser.add_argument("--scorecard-merge", action="store_true", help="Enable Scorecard merge guard (requires --scorecard-crosswalk)")
    parser.add_argument("--scorecard-crosswalk", type=Path, default=None, help="Approved UNITID/OPEID crosswalk for Scorecard merges")
    return parser.parse_args(argv)


def configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(message)s",
    )


def parse_years(expr: str) -> List[int]:
    expr = expr.strip()
    if ":" in expr and "," in expr:
        raise ValueError("Cannot mix range and comma syntax in --years")
    if ":" in expr:
        start_str, end_str = expr.split(":", 1)
        start, end = int(start_str), int(end_str)
        if end < start:
            start, end = end, start
        return list(range(start, end + 1))
    if "," in expr:
        return [int(part.strip()) for part in expr.split(",") if part.strip()]
    return [int(expr)]


def load_dictionary_lake(path: Path) -> pd.DataFrame:
    logging.info("Loading dictionary lake from %s", path)
    try:
        lake = pd.read_parquet(path)
    except (ImportError, ValueError) as exc:
        message = str(exc).lower()
        if "pyarrow" in message or "fastparquet" in message:
            raise ImportError(
                "pyarrow or fastparquet required for parquet support. Run: source .venv/bin/activate && pip install -r requirements.txt"
            ) from exc
        raise
    if "year" not in lake.columns:
        raise KeyError("dictionary lake must include a 'year' column")
    lake["year"] = lake["year"].astype(int)
    for col in ["source_label_norm", "source_label", "prefix_hint", "survey_hint", "release", "dict_file", "filename"]:
        if col in lake.columns:
            lake[col] = lake[col].astype(str)
    return lake


def load_validation_rules(path: Path) -> dict:
    logging.info("Loading validation rules from %s", path)
    with path.open("r", encoding="utf-8") as fh:
        return yaml.safe_load(fh) or {}


def extract_prefixes(row: pd.Series) -> set[str]:
    prefixes: set[str] = set()
    prefix_hint = str(row.get("prefix_hint", "")).strip()
    if prefix_hint and prefix_hint not in {"nan", "None", ""}:
        prefixes.add(prefix_hint.upper())
    filename = str(row.get("filename", ""))
    dict_file = str(row.get("dict_file", ""))
    pattern = re.compile(r"\b(F1A|F2A|F3A|EF|EFIA|E12|E1D|EFFY|SFA|CST|OM|GRS|PE)\b", re.IGNORECASE)
    for text in (filename, dict_file):
        for match in pattern.findall(text):
            prefixes.add(match.upper())
        parent = Path(text).parent.name.upper() if text else ""
        for form in ("EFIA", "EFFY", "E12", "E1D", "SFA", "CST"):
            if parent.startswith(form):
                prefixes.add(form)
    return prefixes


def _parse_income_band(text: str) -> tuple[Optional[int], Optional[int]]:
    raw = (text or "").lower()
    raw = raw.replace("k", "000")
    cleaned = re.sub(r"[,$\s]+", " ", raw)

    def _to_int(token: str) -> int:
        return int(re.sub(r"[\s,]", "", token))

    match = re.search(r"(?:less than|under)\s*(\d{1,3}(?:,\d{3})*)", raw)
    if match:
        hi = _to_int(match.group(1))
        return 0, hi
    match = re.search(r"(0)\s*(?:to|-|–)\s*(\d{1,3}(?:,\d{3})*)", raw)
    if match:
        hi = _to_int(match.group(2))
        return 0, hi
    match = re.search(r"(\d{1,3}(?:,\d{3})*)\s*(?:\+|or more|and above)", raw)
    if match:
        lo = _to_int(match.group(1))
        return lo, None
    target = raw.split("income", 1)[1] if "income" in raw else raw
    match = re.search(r"(\d{1,3}(?:,\d{3})*)\s*(?:to|-|–)\s*(\d{1,3}(?:,\d{3})*)", target)
    if match:
        lo = _to_int(match.group(1))
        hi = _to_int(match.group(2))
        return lo, hi
    return None, None


def _bands_overlap(a: tuple[Optional[int], Optional[int]], b: tuple[Optional[int], Optional[int]]) -> bool:
    lo_a, hi_a = a
    lo_b, hi_b = b
    if lo_a is None and lo_b is None:
        return True
    lo_a = lo_a or 0
    lo_b = lo_b or 0
    hi_a = hi_a if hi_a is not None else float("inf")
    hi_b = hi_b if hi_b is not None else float("inf")
    return max(lo_a, lo_b) <= min(hi_a, hi_b)


def filter_candidates_by_forms(df: pd.DataFrame, forms: Optional[Sequence[str]]) -> pd.DataFrame:
    if not forms:
        return df.copy()
    allowed = {f.upper() for f in forms}
    mask = []
    for _, row in df.iterrows():
        prefixes = extract_prefixes(row)
        mask.append(bool(prefixes & allowed))
    return df.loc[mask].copy()


def score_candidate(row: pd.Series, concept: dict) -> float:
    label_norm_raw = str(row.get("source_label_norm") or row.get("source_label") or "").strip()
    label_norm_stripped = re.sub(r"[,;$%]", "", label_norm_raw)
    label_norm = label_norm_stripped.lower()

    if METADATA_NEGATIVES.match(label_norm_stripped):
        return -5.0

    score = 0.0
    matched = False
    for pattern in concept.get("label_regex", []):
        regex = re.compile(pattern, re.IGNORECASE)
        if regex.fullmatch(label_norm) or regex.fullmatch(label_norm_stripped) or regex.fullmatch(label_norm_raw):
            score = max(score, 4.0)
            matched = True
            break
        if (
            regex.search(label_norm)
            or regex.search(label_norm_stripped)
            or regex.search(label_norm_raw)
        ):
            score = max(score, 2.0)
            matched = True
    if not matched:
        score -= 1.0
    required_keywords = [
        tok.strip().lower()
        for tok in (concept.get("requires_keywords") or [])
        if isinstance(tok, str) and tok.strip()
    ]
    if required_keywords and not all(tok in label_norm for tok in required_keywords):
        score = min(score, 3.0)
    bonus = 0.0
    forms = concept.get("forms")
    prefixes = extract_prefixes(row)
    if forms:
        allowed = {f.upper() for f in forms}
        if prefixes & allowed:
            bonus = 1.0
        else:
            survey_hint = str(row.get("survey_hint", "")).strip().lower()
            if survey_hint and survey_hint == str(concept.get("survey", "")).strip().lower():
                bonus = 1.0
    else:
        survey_hint = str(row.get("survey_hint", "")).strip().lower()
        if survey_hint and survey_hint == str(concept.get("survey", "")).strip().lower():
            bonus = 1.0
    score += bonus
    units = _to_lower(str(concept.get("units", "")))
    money_hint = bool(re.search(r"\b(dollars?|amount|revenue|expenses?|net price)\b|\$", label_norm))
    count_hint = bool(re.search(r"\bcount|students?|recipients?|headcount|fte\b", label_norm))
    if units in {"usd", "$"}:
        if money_hint:
            score += 0.5
        if count_hint:
            score -= 0.5
    if units in {"count", "fte"}:
        if count_hint:
            score += 0.5
        if money_hint:
            score -= 0.5
    if "band_min" in concept or "band_max" in concept:
        parsed = _parse_income_band(label_norm_raw)
        if parsed != (None, None):
            lo = concept.get("band_min")
            hi = concept.get("band_max")
            if _bands_overlap(parsed, (lo, hi)):
                score = max(score, 4.0)
            else:
                score -= 1.0
    for pattern in concept.get("exclude_regex", []):
        if re.search(pattern, label_norm, flags=re.IGNORECASE) or re.search(
            pattern, label_norm_raw, flags=re.IGNORECASE
        ):
            score -= 2.0
    return score


def choose_candidate(
    df: pd.DataFrame, concept_key: str, concept: dict
) -> tuple[Optional[pd.Series], float, list[tuple[float, pd.Series]], int]:
    if df.empty:
        return None, float("nan"), [], 0

    filtered_rows: list[pd.Series] = []
    survey_lower = str(concept.get("survey", "")).strip().lower()
    for _, row in df.iterrows():
        source_var = str(row.get("source_var") or "")
        label_norm = str(row.get("source_label_norm") or row.get("source_label") or "")
        if GLOBAL_EXCLUDE.search(source_var) or GLOBAL_EXCLUDE.search(label_norm):
            continue
        if survey_lower == "finance" and FINANCE_STOCK_EXCLUDE.search(label_norm):
            continue
        filtered_rows.append(row)

    if not filtered_rows:
        return None, float("nan"), [], 0

    scored_rows: list[tuple[float, pd.Series]] = []
    for row in filtered_rows:
        score = score_candidate(row, concept)
        scored_rows.append((score, row))

    scored_rows.sort(
        key=lambda item: (
            -item[0],
            -1 if str(item[1].get("release", "")).lower() == "revised" else 0,
            -(len(str(item[1].get("source_label_norm") or item[1].get("source_label") or ""))),
        )
    )

    if not scored_rows:
        return None, float("nan"), [], 0

    top_candidates = scored_rows[:TOPK_AUDIT]
    best_score, best_row = top_candidates[0]

    if pd.isna(best_score) or best_score < MIN_ACCEPT_SCORE:
        return None, best_score, top_candidates, len(filtered_rows)

    logging.info(
        "Resolved concept %s with source_var=%s label='%s' score=%.2f release=%s",
        concept_key,
        best_row.get("source_var"),
        best_row.get("source_label"),
        best_score,
        best_row.get("release"),
    )
    return best_row, best_score, top_candidates, len(filtered_rows)


def coerce_optional_str(value: object) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        cleaned = value.strip()
        if not cleaned or cleaned.lower() in {"nan", "none"}:
            return None
        return cleaned
    if pd.isna(value):
        return None
    text = str(value).strip()
    if not text or text.lower() in {"nan", "none"}:
        return None
    return text


def format_top_alternates(candidates: list[tuple[float, pd.Series]]) -> str:
    parts: list[str] = []
    for score, row in candidates[1:]:
        source_var = coerce_optional_str(row.get("source_var")) or ""
        if not source_var:
            continue
        label = coerce_optional_str(row.get("source_label")) or coerce_optional_str(
            row.get("source_label_norm")
        )
        score_str = "" if pd.isna(score) else f"{score:.2f}"
        parts.append(f"{source_var}|{score_str}|{label or ''}")
    return ";".join(parts)


def _form_priority(prefix: Optional[str]) -> int:
    order = {"F1A": 3, "F2A": 2, "F3A": 1}
    return order.get((prefix or "").upper(), 0)


def resolve_crossform_conflicts(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:
    if df.empty:
        return df, df.iloc[0:0].copy()
    key = ["UNITID", "year", "survey", "target_var"]
    if "state" in df.columns:
        key.append("state")
    dup_mask = df.duplicated(key, keep=False)
    if not dup_mask.any():
        return df, df.iloc[0:0].copy()
    work = df.copy()
    work["release_rank"] = work["release"].astype(str).str.lower().eq("revised").astype(int)
    work["score_rank"] = pd.to_numeric(work.get("decision_score"), errors="coerce").fillna(-9e9)
    work["form_rank"] = work["form_family"].map(_form_priority).fillna(0)
    work = work.sort_values(
        key + ["release_rank", "score_rank", "form_rank", "source_file"],
        ascending=[True, True, True, False, False, False, True],
    )
    keep_idx = work.groupby(key, as_index=False).head(1).index
    conflicts = work.loc[dup_mask].copy()
    cleaned = work.loc[work.index.isin(keep_idx)].copy()
    for col in ["release_rank", "score_rank", "form_rank"]:
        cleaned.drop(columns=[col], inplace=True, errors="ignore")
        conflicts.drop(columns=[col], inplace=True, errors="ignore")
    return cleaned, conflicts


def determine_prefix(row: Optional[pd.Series], concept: dict) -> Optional[str]:
    if row is None:
        forms = concept.get("forms")
        if forms and len(forms) == 1:
            return forms[0].upper()
        return None
    prefixes = extract_prefixes(row)
    forms = concept.get("forms")
    if forms:
        allowed = [f.upper() for f in forms]
        for form in allowed:
            if form in prefixes:
                return form
        if len(allowed) == 1:
            return allowed[0]
    if prefixes:
        return sorted(prefixes)[0]
    return None


def load_manifest(year: int, root: Path, cache: dict[int, Optional[pd.DataFrame]] = None) -> Optional[pd.DataFrame]:
    if cache is None:
        cache = {}
    if year in cache:
        return cache[year]
    manifest_path = root / str(year) / f"{year}_manifest.csv"
    if not manifest_path.exists():
        logging.warning("Manifest %s missing; will fallback to filesystem scan", manifest_path)
        cache[year] = None
        return None
    manifest = pd.read_csv(manifest_path, dtype={"is_revision": str})
    cache[year] = manifest
    return manifest


def prefer_manifest_row(
    manifest: pd.DataFrame, prefix: str, survey: str, dict_hint: Optional[str] = None
) -> Optional[pd.Series]:
    if manifest is None or manifest.empty:
        return None
    prefix_upper = prefix.upper()
    subset = manifest[manifest["prefix"].astype(str).str.upper() == prefix_upper]
    if subset.empty:
        subset = manifest[manifest["filename"].astype(str).str.contains(prefix_upper, case=False, na=False)]
    if subset.empty:
        return None

    if dict_hint and "dictionary_filename" in subset.columns:
        dict_base = os.path.basename(str(dict_hint))
        with_dict = subset[subset["dictionary_filename"].astype(str) == dict_base]
        if not with_dict.empty:
            subset = with_dict

    subset = subset.copy()
    if "is_revision" in subset.columns:
        subset["is_revision_flag"] = subset["is_revision"].astype(str).str.lower().isin(["true", "1", "yes"])
    else:
        subset["is_revision_flag"] = False
    if "release" in subset.columns:
        subset["release_rank"] = subset["release"].astype(str).str.lower().eq("revised").astype(int)
    else:
        subset["release_rank"] = 0
    subset["prefix_exact"] = subset["prefix"].astype(str).str.upper().eq(prefix_upper)
    subset.sort_values(
        ["prefix_exact", "is_revision_flag", "release_rank"],
        ascending=[False, False, False],
        inplace=True,
    )
    return subset.iloc[0]


def locate_data_file(
    year: int,
    prefix: Optional[str],
    survey: str,
    root: Path,
    manifest_cache: dict[int, Optional[pd.DataFrame]],
    dict_hint: Optional[str] = None,
) -> tuple[Optional[Path], Optional[str]]:
    if prefix is None:
        logging.warning("Cannot locate data file without prefix for year=%s survey=%s", year, survey)
        return None, None
    manifest = load_manifest(year, root, manifest_cache)
    manifest_row = (
        prefer_manifest_row(manifest, prefix, survey, dict_hint) if manifest is not None else None
    )
    if manifest_row is not None:
        filename = manifest_row.get("filename")
        release = manifest_row.get("release")
        if filename:
            path = root / str(year) / str(filename)
            if path.exists():
                return path, str(release) if release is not None else None
        logging.warning("Manifest pointed to %s but file missing; falling back", filename)
    year_dir = root / str(year)
    if not year_dir.exists():
        logging.error("Year directory %s missing", year_dir)
        return None, None
    candidates: list[Path] = []
    seen: set[Path] = set()
    prefix_lower = prefix.lower()
    allowed_suffixes = {".csv", ".tsv", ".txt", ".parquet", ".xlsx", ".xls"}
    for file in year_dir.rglob("*"):
        if not file.is_file():
            continue
        if file.suffix.lower() not in allowed_suffixes:
            continue
        name_lower = file.name.lower()
        parent_lower = file.parent.name.lower()
        if prefix_lower in name_lower or prefix_lower in parent_lower:
            resolved = file.resolve()
            if resolved in seen:
                continue
            seen.add(resolved)
            candidates.append(file)
    if not candidates:
        logging.warning("No files found matching prefix %s in %s", prefix, year_dir)
        return None, None

    dict_tokens: set[str] = set()
    preferred_dir: Optional[str] = None
    if dict_hint:
        dict_path = Path(str(dict_hint))
        dict_stem = dict_path.stem
        dict_stem = re.sub(r"(?i)(dict|dictionary)", "", dict_stem)
        dict_tokens = {tok for tok in re.findall(r"[A-Z0-9]+", dict_stem.upper()) if tok}
        parent_name = dict_path.parent.name
        if parent_name:
            preferred_dir = re.sub(r"(?i)(?:[_-]?dict|dictionary)$", "", parent_name).upper()

    def rank(p: Path) -> tuple[int, int, float]:
        name = p.name.upper()
        overlap = sum(1 for t in dict_tokens if t in name) if dict_tokens else 0
        text_pref = 1 if p.suffix.lower() in {".csv", ".tsv", ".txt"} else 0
        parent = p.parent.name.upper()
        dir_boost = 0
        if preferred_dir:
            if parent == preferred_dir:
                dir_boost = 2
            elif parent.startswith(preferred_dir):
                dir_boost = 1
        prefix_hit = 1 if prefix.lower() in name else 0
        return (prefix_hit * 3 + overlap + dir_boost, text_pref, p.stat().st_mtime)

    candidates.sort(key=rank, reverse=True)
    chosen = candidates[0]
    logging.info("Fallback located %s for prefix %s year %s", chosen, prefix, year)
    return chosen, None


def load_data_file(path: Path, cache: dict[Path, tuple[pd.DataFrame, Optional[str]]]) -> tuple[pd.DataFrame, Optional[str]]:
    if path in cache:
        return cache[path]
    suffix = path.suffix.lower()
    if suffix in {".csv", ".tsv", ".txt"}:
        if suffix == ".txt":
            try:
                df = pd.read_csv(path, dtype=str, sep=None, engine="python", na_filter=False, low_memory=False)
            except Exception:
                try:
                    df = pd.read_csv(path, dtype=str, sep="|", na_filter=False, low_memory=False)
                except Exception:
                    df = pd.read_csv(path, dtype=str, delim_whitespace=True, na_filter=False, low_memory=False)
        else:
            sep = "," if suffix == ".csv" else "\t"
            try:
                df = pd.read_csv(path, dtype=str, sep=sep, na_filter=False, low_memory=False)
            except UnicodeDecodeError:
                logging.warning("Non-UTF8 content detected in %s; retrying with latin-1 encoding", path)
                df = pd.read_csv(
                    path,
                    dtype=str,
                    sep=sep,
                    na_filter=False,
                    low_memory=False,
                    encoding="latin1",
                    encoding_errors="replace",
                )
    elif suffix == ".parquet":
        try:
            df = pd.read_parquet(path)
        except (ImportError, ValueError) as exc:
            message = str(exc).lower()
            if "pyarrow" in message or "fastparquet" in message:
                raise ImportError(
                    "pyarrow or fastparquet required for parquet support. Run: source .venv/bin/activate && pip install -r requirements.txt"
                ) from exc
            raise
        df = df.applymap(lambda x: str(x) if not pd.isna(x) else None)
    elif suffix in {".xlsx", ".xls"}:
        engine = "openpyxl" if suffix == ".xlsx" else "xlrd"
        df = pd.read_excel(path, dtype=str, engine=engine)
    else:
        raise ValueError(f"Unsupported file type: {suffix}")
    df.columns = [str(col).strip() for col in df.columns]
    unitid_col = None
    for col in df.columns:
        if col.lower() == "unitid":
            unitid_col = col
            break
    if unitid_col is None:
        logging.warning("UNITID column missing in %s", path)
    else:
        df[unitid_col] = pd.to_numeric(df[unitid_col], errors="coerce").astype("Int64")
    cache[path] = (df, unitid_col)
    return cache[path]


def find_source_column(df: pd.DataFrame, source_var: str) -> Optional[str]:
    if source_var in df.columns:
        return source_var
    lookup = {col.lower(): col for col in df.columns}
    return lookup.get(source_var.lower())


def coerce_numeric(series: pd.Series) -> pd.Series:
    if series.dtype.kind in {"i", "u", "f"}:
        return series.astype(float)
    replaced = series.replace(NA_TOKENS, pd.NA)
    return pd.to_numeric(replaced, errors="coerce")


def apply_transform(values: pd.Series, transform: str) -> pd.Series:
    transform = (transform or "identity").strip().lower()
    if transform == "identity":
        return values
    if transform.startswith("scale:"):
        factor = float(transform.split(":", 1)[1])
        return values * factor
    if transform.startswith("divide:"):
        divisor = float(transform.split(":", 1)[1])
        return values / divisor
    if transform == "percent":
        return values * 100.0
    if transform == "negate":
        return -values
    logging.warning("Unknown transform '%s'; applying identity", transform)
    return values


def resolve_imputation_flags(df: pd.DataFrame, source_col: str) -> tuple[pd.Series, pd.Series, Optional[str]]:
    """Return imputed flag series, boolean mask, and chosen flag column for a source column."""
    impute_lookup = {str(col).lower(): col for col in df.columns}
    candidate_imputed_cols: list[str] = []
    xvar = f"X{source_col}"
    if xvar.lower() in impute_lookup:
        candidate_imputed_cols.append(impute_lookup[xvar.lower()])
    exact_key = f"x{source_col}".lower()
    if exact_key in impute_lookup and impute_lookup[exact_key] not in candidate_imputed_cols:
        candidate_imputed_cols.append(impute_lookup[exact_key])
    if not candidate_imputed_cols:
        pattern_cols = [col for col in df.columns if IMPUTE_FLAG_PATTERN.search(str(col))]
        containing = [
            col
            for col in pattern_cols
            if source_col.lower() in str(col).lower()
        ]
        if containing:
            candidate_imputed_cols.extend(containing)
        elif pattern_cols:
            candidate_imputed_cols.append(pattern_cols[0])
    imputed_col = candidate_imputed_cols[0] if candidate_imputed_cols else None
    if imputed_col is not None:
        imputed_values = df[imputed_col]
    else:
        imputed_values = pd.Series(pd.NA, index=df.index)
    normalized_flags = imputed_values.astype("string").str.strip().str.lower()
    not_imputed_tokens = {"", "0", "n", "na", "nan", "none", "null", "no", "false"}
    missing_mask = normalized_flags.isna()
    false_mask = normalized_flags.isin(not_imputed_tokens)
    bool_mask = (~missing_mask) & (~false_mask)
    is_imputed = bool_mask.astype("boolean").mask(missing_mask, pd.NA)
    return imputed_values, is_imputed, imputed_col


def run_nonnegative_rule(df: pd.DataFrame, rule: dict) -> int:
    vars_set = set(rule.get("target_vars", []))
    subset = df[df["target_var"].isin(vars_set)]
    violations = subset[subset["value"] < 0].shape[0]
    return violations


def run_balance_rule(df: pd.DataFrame, rule: dict) -> int:
    total_var = rule.get("total")
    part_vars = rule.get("parts", [])
    tolerance = float(rule.get("tolerance", 0))
    pivot = df[df["target_var"].isin([total_var] + list(part_vars))]
    if pivot.empty:
        return 0
    pivot = pivot.pivot_table(
        index=["UNITID", "year"],
        columns="target_var",
        values="value",
        aggfunc="first",
    )
    violations = 0
    for _, row in pivot.iterrows():
        total = row.get(total_var)
        parts = row[list(part_vars)].sum(min_count=1)
        if pd.isna(total) or pd.isna(parts):
            continue
        if total + abs(total) * tolerance < parts:
            violations += 1
    return violations


def run_growth_rule(df: pd.DataFrame, rule: dict) -> int:
    target_var = rule.get("target_var")
    max_abs_pct = float(rule.get("max_abs_pct", 0))
    subset = df[df["target_var"] == target_var].dropna(subset=["UNITID", "year", "value"])
    if subset.empty:
        return 0
    subset = subset.sort_values(["UNITID", "year"])
    subset["lag"] = subset.groupby("UNITID")["value"].shift(1)
    subset = subset.dropna(subset=["lag"])
    subset["pct_change"] = (subset["value"] - subset["lag"]) / subset["lag"].replace(0, pd.NA)
    subset = subset.dropna(subset=["pct_change"])
    violations = subset[subset["pct_change"].abs() > max_abs_pct].shape[0]
    return violations


def run_release_policy(df: pd.DataFrame, rule: dict, strict: bool) -> tuple[int, List[str]]:
    allow_mixed = bool(rule.get("allow_mixed_release", True))
    if allow_mixed:
        return 0, []
    violations = 0
    errors: List[str] = []
    grouped = df.groupby(["target_var", "UNITID"])
    for (target_var, unitid), group in grouped:
        releases = set(group["release"].dropna().str.lower())
        if len(releases) > 1:
            violations += 1
            if strict:
                errors.append(
                    f"Mixed release statuses {releases} for target_var={target_var} UNITID={unitid}"
                )
    return violations, errors


def run_validations(df: pd.DataFrame, rules: dict, strict_release: bool) -> tuple[pd.DataFrame, List[str]]:
    records = []
    errors: List[str] = []
    nonneg_rules = rules.get("nonnegatives", []) or []
    for rule in nonneg_rules:
        violations = run_nonnegative_rule(df, rule)
        records.append(
            {
                "rule_type": "nonnegative",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    balance_rules = rules.get("balances", []) or []
    for rule in balance_rules:
        violations = run_balance_rule(df, rule)
        records.append(
            {
                "rule_type": "balance",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    growth_rules = rules.get("growth_caps", []) or []
    for rule in growth_rules:
        violations = run_growth_rule(df, rule)
        records.append(
            {
                "rule_type": "growth_cap",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    uniform_rules = rules.get("uniform_form_families", []) or []
    for rule in uniform_rules:
        violations = run_uniform_form_rule(df, rule)
        records.append(
            {
                "rule_type": "uniform_form",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    release_rule = rules.get("release_policy", {}) or {}
    violations, release_errors = run_release_policy(df, release_rule, strict_release)
    records.append(
        {
            "rule_type": "release_policy",
            "description": release_rule.get("description"),
            "violations": violations,
        }
    )
    errors.extend(release_errors)
    report = pd.DataFrame(records)
    return report, errors


def _expand_static_target(var_df: pd.DataFrame, target_var: str, years: Sequence[int]) -> pd.DataFrame:
    ordered_years = sorted({int(y) for y in years})
    if not ordered_years or var_df.empty:
        return var_df.copy()
    working = var_df.dropna(subset=["UNITID"]).copy()
    if working.empty:
        return var_df.iloc[0:0].copy()
    working["UNITID"] = working["UNITID"].astype("Int64")
    working = working.sort_values(["UNITID", "year"])
    unitids = working["UNITID"].unique()
    idx = pd.MultiIndex.from_product([unitids, ordered_years], names=["UNITID", "year"])
    dedup = working.drop_duplicates(subset=["UNITID", "year"], keep="first").set_index(["UNITID", "year"])
    expanded = dedup.reindex(idx).reset_index()
    for col in OUTPUT_COLUMNS:
        if col not in expanded.columns:
            expanded[col] = pd.NA
    expanded["target_var"] = target_var
    concept_meta = CONCEPT_BY_TARGET_VAR.get(target_var, {})
    for field in ("concept", "units", "survey", "period_type", "notes"):
        if concept_meta.get(field) is not None:
            expanded[field] = concept_meta.get(field)
    expanded = expanded.sort_values(["UNITID", "year"])
    fill_cols = [
        "value",
        "form_family",
        "source_var",
        "label_matched",
        "imputed_flag",
        "is_imputed",
        "source_file",
        "release",
    ]
    grouped = expanded.groupby("UNITID", group_keys=False)
    for col in fill_cols:
        if col in expanded.columns:
            expanded[col] = grouped[col].transform(lambda s: s.ffill().bfill())
    expanded["value"] = pd.to_numeric(expanded["value"], errors="coerce")
    if "is_imputed" in expanded.columns:
        expanded["is_imputed"] = expanded["is_imputed"].astype("boolean")
    expanded = expanded[OUTPUT_COLUMNS]
    return expanded


def backfill_static_locational_fields(df: pd.DataFrame, years: Sequence[int]) -> pd.DataFrame:
    if df.empty:
        return df
    ordered_years = sorted({int(y) for y in years})
    if not ordered_years:
        return df
    mask = df["target_var"].isin(STATIC_LOCATIONAL_TARGETS)
    if not mask.any():
        return df
    static_df = df[mask].copy()
    rest_df = df[~mask].copy()
    filled_frames: List[pd.DataFrame] = []
    for target_var, target_df in static_df.groupby("target_var"):
        filled = _expand_static_target(target_df, target_var, ordered_years)
        filled_frames.append(filled)
    combined = pd.concat([rest_df] + filled_frames, ignore_index=True)
    combined = combined[OUTPUT_COLUMNS]
    return combined


def build_output_frame(records: List[pd.DataFrame]) -> pd.DataFrame:
    if not records:
        return pd.DataFrame(columns=OUTPUT_COLUMNS)
    df = pd.concat(records, ignore_index=True)
    columns = OUTPUT_COLUMNS.copy()
    if "state" in df.columns and "state" not in columns:
        columns.insert(1, "state")
    if "reporting_unitid" not in df.columns:
        df["reporting_unitid"] = df.get("UNITID")
    df = df[columns]
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    return df


def run_uniform_form_rule(df: pd.DataFrame, rule: dict) -> int:
    survey = str(rule.get("survey") or "").strip().lower()
    if not survey:
        return 0
    sub = df[df["survey"].astype(str).str.lower() == survey]
    if sub.empty:
        return 0
    mix = (
        sub.groupby(["UNITID", "year"])["form_family"]
        .nunique(dropna=True)
    )
    return int((mix > 1).sum())


def load_reporting_map(path: Optional[Path]) -> Optional[pd.DataFrame]:
    if not path:
        return None
    if not path.exists():
        logging.error("Reporting map %s not found", path)
        return None
    df = pd.read_csv(path, dtype={"UNITID": "Int64", "reporting_unitid": "Int64", "component": str, "action": str})
    df["component"] = df["component"].str.strip().str.lower()
    df["action"] = df["action"].str.strip().str.lower()
    return df


def apply_reporting_rules(frame: pd.DataFrame, survey: str, rpt_map: Optional[pd.DataFrame]) -> pd.DataFrame:
    if frame.empty:
        return frame
    if rpt_map is None:
        if "reporting_unitid" not in frame.columns:
            frame["reporting_unitid"] = frame["UNITID"]
        frame["reporting_map_policy"] = frame.get("reporting_map_policy", "keep_child")
        frame["reporting_unit_scope"] = frame.get("reporting_unit_scope", "campus")
        frame["allocation_factor_used"] = frame.get("allocation_factor_used", "")
        return frame
    comp = (survey or "").strip().lower()
    rules = rpt_map[rpt_map["component"].eq(comp)]
    if rules.empty:
        if "reporting_unitid" not in frame.columns:
            frame["reporting_unitid"] = frame["UNITID"]
        frame["reporting_map_policy"] = frame.get("reporting_map_policy", "keep_child")
        frame["reporting_unit_scope"] = frame.get("reporting_unit_scope", "campus")
        frame["allocation_factor_used"] = frame.get("allocation_factor_used", "")
        return frame
    base = frame.copy()
    base["reporting_map_policy"] = "keep_child"
    base["reporting_unit_scope"] = "campus"
    base["allocation_factor_used"] = base.get("allocation_factor_used", "")
    rules = rules[["UNITID", "reporting_unitid", "action"]].dropna(how="all")
    merged = base.merge(rules, how="left", on="UNITID", suffixes=("", "_rule"))
    merged["reporting_unitid"] = merged["reporting_unitid"].fillna(merged["UNITID"]).astype("Int64")
    merged["action"] = merged["action"].fillna("keep_child")
    merged["reporting_map_policy"] = merged["action"]
    drop_mask = merged["action"].eq("drop_child") & merged["UNITID"].ne(merged["reporting_unitid"])
    merged = merged.loc[~drop_mask].copy()
    roll_mask = merged["action"].eq("roll_to_parent") & merged["reporting_unitid"].notna()
    merged.loc[roll_mask, "UNITID"] = merged.loc[roll_mask, "reporting_unitid"]
    merged.loc[roll_mask, "reporting_unit_scope"] = "system"
    merged.drop(columns=["action"], inplace=True)
    return merged


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    configure_logging(args.log_level)
    years = parse_years(args.years)
    report_duplicate_modules()
    if args.scorecard_merge and not args.scorecard_crosswalk:
        raise SystemExit("Refusing to merge on OPEID without --scorecard-crosswalk. See README.")
    lake = load_dictionary_lake(args.lake)
    rules = load_validation_rules(args.rules)
    reporting_map = load_reporting_map(args.reporting_map)

    lake_by_year: dict[int, pd.DataFrame] = {year: lake[lake["year"] == year].copy() for year in sorted(lake["year"].unique())}
    manifest_cache: dict[int, Optional[pd.DataFrame]] = {}
    data_cache: dict[Path, tuple[pd.DataFrame, Optional[str]]] = {}
    decision_records: List[CandidateSelection] = []
    output_frames: List[pd.DataFrame] = []

    for year in years:
        year_lake = lake_by_year.get(year, pd.DataFrame(columns=lake.columns))
        logging.info("Processing year %s with %d dictionary rows", year, len(year_lake))
        for concept_key, concept in CONCEPTS.items():
            family = str(concept.get("family") or "")
            record_kwargs = {
                "concept_key": concept_key,
                "year": year,
                "target_var": str(concept.get("target_var")),
                "score": None,
                "n_candidates": 0,
                "source_var": None,
                "label_matched": None,
                "dict_file": None,
                "dict_filename": None,
                "prefix_hint": None,
                "survey_hint": None,
                "chosen_data_file": None,
                "manifest_release": None,
                "notes": concept.get("notes"),
                "top_alternates": None,
                "imputed_flag": None,
                "accepted": None,
            }
            min_year = concept.get("min_year")
            max_year = concept.get("max_year")
            if min_year and year < int(min_year):
                note = concept.get("notes") or ""
                reason = f"Unavailable before {min_year}"
                record_kwargs["notes"] = f"{note} - {reason}" if note else reason
                record_kwargs["accepted"] = True
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            if max_year and year > int(max_year):
                note = concept.get("notes") or ""
                reason = f"Retired after {max_year}"
                record_kwargs["notes"] = f"{note} - {reason}" if note else reason
                record_kwargs["accepted"] = True
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            candidates_df = filter_candidates_by_forms(year_lake, concept.get("forms"))
            raw_candidates = len(candidates_df)
            best_row: Optional[pd.Series]
            best_score: float
            top_candidates: list[tuple[float, pd.Series]]
            candidate_count = 0
            if raw_candidates:
                best_row, best_score, top_candidates, candidate_count = choose_candidate(
                    candidates_df, concept_key, concept
                )
            else:
                best_row, best_score, top_candidates = None, float("nan"), []
            top_alternates = format_top_alternates(top_candidates)
            primary_row = best_row if best_row is not None else (top_candidates[0][1] if top_candidates else None)
            dict_file = coerce_optional_str(primary_row.get("dict_file")) if primary_row is not None else None
            dict_filename = os.path.basename(dict_file) if dict_file else None
            prefix_hint = coerce_optional_str(primary_row.get("prefix_hint")) if primary_row is not None else None
            survey_hint = coerce_optional_str(primary_row.get("survey_hint")) if primary_row is not None else None
            label_matched = coerce_optional_str(best_row.get("source_label")) if best_row is not None else None
            source_var_val = coerce_optional_str(best_row.get("source_var")) if best_row is not None else None
            score_val = None if pd.isna(best_score) else float(best_score)
            survey_name = str(concept.get("survey", ""))
            record_kwargs.update(
                {
                    "score": score_val,
                    "n_candidates": candidate_count,
                    "source_var": source_var_val,
                    "label_matched": label_matched,
                    "dict_file": dict_file,
                    "dict_filename": dict_filename,
                    "prefix_hint": prefix_hint,
                    "survey_hint": survey_hint,
        "top_alternates": top_alternates,
            }
        )
            if best_row is None:
                logging.warning(
                    "Weak or no match for %s in %s (score=%.2f; candidates=%s)",
                    concept_key,
                    year,
                    float(best_score) if pd.notna(best_score) else float("nan"),
                    candidate_count,
                )
                record_kwargs["accepted"] = False
                record_kwargs["accepted"] = False
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            prefix = determine_prefix(best_row, concept)
            finance_basis = ""
            if survey_name.strip().lower() == "finance":
                form_upper = (prefix or "").upper()
                if form_upper.startswith("F1"):
                    finance_basis = "GASB"
                elif form_upper.startswith("F2"):
                    finance_basis = "FASB"
                elif form_upper.startswith("F3"):
                    finance_basis = "FORPROFIT"
            release = coerce_optional_str(best_row.get("release"))
            data_path, manifest_release = locate_data_file(
                year,
                prefix,
                str(concept.get("survey", "")),
                args.root,
                manifest_cache,
                dict_hint=dict_file,
            )
            if manifest_release:
                record_kwargs["manifest_release"] = manifest_release
            if data_path is not None:
                record_kwargs["chosen_data_file"] = str(data_path)
            effective_release = manifest_release or release
            if data_path is None:
                logging.warning(
                    "No data file found for concept %s year %s prefix %s", concept_key, year, prefix
                )
                record_kwargs["accepted"] = False
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            df, unitid_col = load_data_file(data_path, data_cache)
            if unitid_col is None:
                logging.warning(
                    "Skipping concept %s year %s due to missing UNITID in %s", concept_key, year, data_path
                )
                record_kwargs["accepted"] = False
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            source_var = source_var_val or ""
            source_col = find_source_column(df, source_var)
            if source_col is None:
                logging.warning(
                    "Source var %s not present in %s for concept %s year %s",
                    source_var,
                    data_path,
                    concept_key,
                    year,
                )
                record_kwargs["accepted"] = False
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            if family == "state_residence":
                row_patterns = [re.compile(p, re.IGNORECASE) for p in concept.get("row_regex", []) if p]
                exclude_patterns = [re.compile(p, re.IGNORECASE) for p in concept.get("exclude_regex", []) if p]
                fam_df = year_lake.copy()
                if dict_file:
                    fam_df = fam_df[fam_df["dict_file"] == dict_file]
                fam_df = filter_candidates_by_forms(fam_df, concept.get("forms"))
                state_rows: dict[str, pd.Series] = {}
                for _, fam_row in fam_df.iterrows():
                    label_raw = str(fam_row.get("source_label_norm") or fam_row.get("source_label") or "")
                    label_norm = label_raw.lower()
                    if row_patterns and not any(pattern.search(label_norm) for pattern in row_patterns):
                        continue
                    state_token = _extract_state_token(label_norm)
                    if not state_token:
                        continue
                    if any(pattern.search(label_norm) for pattern in exclude_patterns):
                        continue
                    state_rows.setdefault(state_token, fam_row)
                if not state_rows:
                    record_kwargs["accepted"] = False
                    decision_records.append(CandidateSelection(**record_kwargs))
                    continue
                long_frames: list[pd.DataFrame] = []
                stored_imputed_name: Optional[str] = record_kwargs.get("imputed_flag")
                for state_token, fam_row in state_rows.items():
                    row_source = coerce_optional_str(fam_row.get("source_var")) or source_col
                    state_col = find_source_column(df, row_source)
                    if state_col is None:
                        continue
                    vals = coerce_numeric(df[state_col])
                    vals = apply_transform(vals, str(concept.get("transform", "identity")))
                    imputed_values, is_imputed, imputed_col_name = resolve_imputation_flags(df, state_col)
                    if imputed_col_name and not stored_imputed_name:
                        record_kwargs["imputed_flag"] = imputed_col_name
                        stored_imputed_name = imputed_col_name
                    state_label = coerce_optional_str(fam_row.get("source_label")) or coerce_optional_str(
                        fam_row.get("source_label_norm")
                    )
                    frame = pd.DataFrame(
                        {
                            "UNITID": df[unitid_col],
                            "reporting_unitid": df[unitid_col],
                            "state": state_token,
                            "year": year,
                            "target_var": concept.get("target_var"),
                            "value": vals,
                            "concept": concept.get("concept"),
                            "units": concept.get("units"),
                            "survey": concept.get("survey"),
                            "form_family": prefix,
                            "finance_basis": finance_basis,
                            "decision_score": score_val,
                            "period_type": concept.get("period_type"),
                            "source_var": state_col,
                            "label_matched": state_label or label_matched,
                            "imputed_flag": imputed_values,
                            "is_imputed": is_imputed,
                            "source_file": str(data_path),
                            "release": effective_release,
                            "notes": concept.get("notes"),
                            "reporting_map_policy": "",
                            "reporting_unit_scope": "campus",
                            "allocation_factor_used": "",
                        }
                    )
                    frame = apply_reporting_rules(frame, str(concept.get("survey", "")), reporting_map)
                    long_frames.append(frame)
                if long_frames:
                    output_frames.extend(long_frames)
                    record_kwargs["accepted"] = True
                else:
                    record_kwargs["accepted"] = False
                decision_records.append(CandidateSelection(**record_kwargs))
                continue

            values = coerce_numeric(df[source_col])
            values = apply_transform(values, str(concept.get("transform", "identity")))
            imputed_values, is_imputed, imputed_col = resolve_imputation_flags(df, source_col)
            if imputed_col is not None:
                record_kwargs["imputed_flag"] = str(imputed_col)

            frame = pd.DataFrame(
                {
                    "UNITID": df[unitid_col],
                    "reporting_unitid": df[unitid_col],
                    "year": year,
                    "target_var": concept.get("target_var"),
                    "value": values,
                    "concept": concept.get("concept"),
                    "units": concept.get("units"),
                    "survey": concept.get("survey"),
                    "form_family": prefix,
                    "finance_basis": finance_basis,
                    "decision_score": score_val,
                    "period_type": concept.get("period_type"),
                    "source_var": source_col,
                    "label_matched": label_matched,
                    "imputed_flag": imputed_values,
                    "is_imputed": is_imputed,
                    "source_file": str(data_path),
                    "release": effective_release,
                    "notes": concept.get("notes"),
                    "reporting_map_policy": "",
                    "reporting_unit_scope": "campus",
                    "allocation_factor_used": "",
                }
            )
            output_frames.append(frame)
            record_kwargs["accepted"] = True
            decision_records.append(CandidateSelection(**record_kwargs))

    output_df = build_output_frame(output_frames)
    output_df = backfill_static_locational_fields(output_df, years)
    output_df, conflicts_df = resolve_crossform_conflicts(output_df)
    if not conflicts_df.empty:
        FORM_CONFLICTS_PATH.parent.mkdir(parents=True, exist_ok=True)
        conflicts_df.to_csv(FORM_CONFLICTS_PATH, index=False)
        logging.warning("Form conflicts detected; details written to %s", FORM_CONFLICTS_PATH)
    report_df, errors = run_validations(output_df, rules, args.strict_release)
    coverage = (
        output_df.groupby(["year", "survey"], dropna=False)["target_var"]
        .nunique()
        .reset_index(name="n_concepts")
    )
    COVERAGE_SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)
    coverage.to_csv(COVERAGE_SUMMARY_PATH, index=False)
    logging.info("Coverage by year and survey written to %s", COVERAGE_SUMMARY_PATH)

    logging.info("Writing output parquet to %s", args.output)
    args.output.parent.mkdir(parents=True, exist_ok=True)
    output_df.to_parquet(args.output, index=False, compression="snappy")
    logging.info("Writing label audit to label_matches.csv")
    audit_columns = [
        "year",
        "concept_key",
        "target_var",
        "score",
        "n_candidates",
        "source_var",
        "label_matched",
        "dict_file",
        "dict_filename",
        "prefix_hint",
        "survey_hint",
        "chosen_data_file",
        "manifest_release",
        "notes",
        "top_alternates",
        "imputed_flag",
        "accepted",
    ]
    audit_df = pd.DataFrame([sel.__dict__ for sel in decision_records])
    if audit_df.empty:
        audit_df = pd.DataFrame(columns=audit_columns)
    else:
        audit_df = audit_df[audit_columns]
    if "accepted" not in audit_df:
        audit_df["accepted"] = pd.NA
    mask = audit_df["accepted"].isna()
    if mask.any():
        audit_df.loc[mask, "accepted"] = audit_df.loc[mask, "score"].apply(
            lambda s: s is not None and not pd.isna(s) and float(s) >= MIN_ACCEPT_SCORE
        )
    audit_df["accepted"] = audit_df["accepted"].astype("boolean")
    LABEL_CHECK_DIR.mkdir(parents=True, exist_ok=True)
    audit_df.to_csv(LABEL_MATCH_PATH, index=False)
    logging.info("Label audit written to %s", LABEL_MATCH_PATH)
    CHECKS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    report_df.to_csv(VALIDATION_REPORT_PATH, index=False)
    logging.info("Validation report written to %s", VALIDATION_REPORT_PATH)

    coverage_fail = False
    if args.strict_coverage:
        weak = audit_df[(audit_df["year"].isin(years)) & (~audit_df["accepted"])]
        if not weak.empty:
            coverage_fail = True
            logging.error(
                "Strict coverage failed for %d rows. Examples:\n%s",
                len(weak),
                weak.head(10).to_string(index=False),
            )
    if errors or coverage_fail:
        for err in errors:
            logging.error(err)
        if args.strict_release:
            logging.error("Strict release policy violated; exiting with failure")
            return 1
        if coverage_fail:
            logging.error("Strict coverage violated; exiting with failure")
            return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())
