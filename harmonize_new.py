"""Label-driven IPEDS harmonizer.

This module implements the replacement for ``harmonize.py`` using a "Power User"
architecture. It consumes the concept catalog defined in ``concept_catalog.py``,
the dictionary lake produced by ``01_ingest_dictionaries.py``, and the manifest
files generated by ``download_ipeds.py``. Concepts are resolved by matching
normalized dictionary labels, data files are located through the per-year
manifest (with fallbacks), and the resulting long-form panel is validated using
``validation_rules.yaml``.

Example
-------
To harmonize 2017-2018 data after running the downloader and dictionary ingest::

    python harmonize_new.py --root /path/to/raw --lake dictionary_lake.parquet \
        --years 2017:2018 --output panel_long.parquet --strict-release

The script will emit ``panel_long.parquet`` with tidy concept values,
``label_matches.csv`` with the label resolution audit, and
``validation_report.csv`` summarizing post-extraction checks.
"""

from __future__ import annotations

import argparse
import logging
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence

import pandas as pd
import yaml

from concept_catalog import CONCEPTS

MIN_ACCEPT_SCORE = 3.0

METADATA_NEGATIVES = re.compile(
    r"^(unitid(_p)?|unit id|instnm|institution name|year|state|zip|fips|sector)$",
    re.IGNORECASE,
)

NA_TOKENS = [
    "PrivacySuppressed",
    "NULL",
    "NaN",
    "N/A",
    "D",
    "DP",
    "â€”",
    ".",
]

OUTPUT_COLUMNS = [
    "UNITID",
    "year",
    "target_var",
    "value",
    "concept",
    "units",
    "survey",
    "form_family",
    "period_type",
    "source_var",
    "label_matched",
    "source_file",
    "release",
    "notes",
]


@dataclass
class CandidateSelection:
    concept_key: str
    year: int
    target_var: str
    source_var: Optional[str]
    dict_file: Optional[str]
    score: Optional[float]
    prefix: Optional[str]
    release: Optional[str]
    n_candidates: int
    label_matched: Optional[str]


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Label-driven IPEDS harmonizer")
    parser.add_argument("--root", type=Path, default=Path("data/raw"), help="Raw data root containing year folders")
    parser.add_argument("--lake", type=Path, default=Path("dictionary_lake.parquet"), help="Dictionary lake parquet path")
    parser.add_argument("--output", type=Path, default=Path("panel_long.parquet"), help="Output parquet path")
    parser.add_argument(
        "--years",
        type=str,
        default="2004:2024",
        help="Year expression: 'YYYY', 'YYYY:YYYY', or comma list 'YYYY,YYYY'",
    )
    parser.add_argument("--rules", type=Path, default=Path("validation_rules.yaml"), help="Validation rules YAML path")
    parser.add_argument("--strict-release", action="store_true", help="Error if mixed provisional/revised releases")
    parser.add_argument("--log-level", type=str, default="INFO", help="Logging level (e.g., INFO, DEBUG)")
    return parser.parse_args(argv)


def configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(message)s",
    )


def parse_years(expr: str) -> List[int]:
    expr = expr.strip()
    if ":" in expr and "," in expr:
        raise ValueError("Cannot mix range and comma syntax in --years")
    if ":" in expr:
        start_str, end_str = expr.split(":", 1)
        start, end = int(start_str), int(end_str)
        if end < start:
            start, end = end, start
        return list(range(start, end + 1))
    if "," in expr:
        return [int(part.strip()) for part in expr.split(",") if part.strip()]
    return [int(expr)]


def load_dictionary_lake(path: Path) -> pd.DataFrame:
    logging.info("Loading dictionary lake from %s", path)
    lake = pd.read_parquet(path)
    if "year" not in lake.columns:
        raise KeyError("dictionary lake must include a 'year' column")
    lake["year"] = lake["year"].astype(int)
    for col in ["source_label_norm", "source_label", "prefix_hint", "survey_hint", "release", "dict_file", "filename"]:
        if col in lake.columns:
            lake[col] = lake[col].astype(str)
    return lake


def load_validation_rules(path: Path) -> dict:
    logging.info("Loading validation rules from %s", path)
    with path.open("r", encoding="utf-8") as fh:
        return yaml.safe_load(fh) or {}


def extract_prefixes(row: pd.Series) -> set[str]:
    prefixes: set[str] = set()
    prefix_hint = str(row.get("prefix_hint", "")).strip()
    if prefix_hint and prefix_hint not in {"nan", "None", ""}:
        prefixes.add(prefix_hint.upper())
    filename = str(row.get("filename", ""))
    dict_file = str(row.get("dict_file", ""))
    pattern = re.compile(r"\b(F1A|F2A|F3A|EF|E12|E1D|EFFY|SFA|CST|OM|GRS|PE)\b", re.IGNORECASE)
    for text in (filename, dict_file):
        for match in pattern.findall(text):
            prefixes.add(match.upper())
    return prefixes


def filter_candidates_by_forms(df: pd.DataFrame, forms: Optional[Sequence[str]]) -> pd.DataFrame:
    if not forms:
        return df.copy()
    allowed = {f.upper() for f in forms}
    mask = []
    for _, row in df.iterrows():
        prefixes = extract_prefixes(row)
        mask.append(bool(prefixes & allowed))
    return df.loc[mask].copy()


def score_candidate(row: pd.Series, concept: dict) -> float:
    label_norm_raw = str(row.get("source_label_norm") or row.get("source_label") or "").strip()
    label_norm = re.sub(r"[,;$%]", "", label_norm_raw)

    if METADATA_NEGATIVES.match(label_norm):
        return -5.0

    score = 0.0
    matched = False
    for pattern in concept.get("label_regex", []):
        regex = re.compile(pattern, re.IGNORECASE)
        if regex.fullmatch(label_norm) or regex.fullmatch(label_norm_raw):
            score = max(score, 4.0)
            matched = True
            break
        if regex.search(label_norm) or regex.search(label_norm_raw):
            score = max(score, 2.0)
            matched = True
    if not matched:
        score -= 1.0
    bonus = 0.0
    forms = concept.get("forms")
    prefixes = extract_prefixes(row)
    if forms:
        allowed = {f.upper() for f in forms}
        if prefixes & allowed:
            bonus = 1.0
        else:
            survey_hint = str(row.get("survey_hint", "")).strip().lower()
            if survey_hint and survey_hint == str(concept.get("survey", "")).strip().lower():
                bonus = 1.0
    else:
        survey_hint = str(row.get("survey_hint", "")).strip().lower()
        if survey_hint and survey_hint == str(concept.get("survey", "")).strip().lower():
            bonus = 1.0
    score += bonus
    for pattern in concept.get("exclude_regex", []):
        if re.search(pattern, label_norm, flags=re.IGNORECASE) or re.search(
            pattern, label_norm_raw, flags=re.IGNORECASE
        ):
            score -= 2.0
    return score


def choose_candidate(df: pd.DataFrame, concept_key: str, concept: dict) -> tuple[Optional[pd.Series], float]:
    if df.empty:
        return None, float("nan")
    scored_rows = []
    for _, row in df.iterrows():
        score = score_candidate(row, concept)
        scored_rows.append((score, row))
    scored_rows.sort(
        key=lambda item: (
            -item[0],
            -1 if str(item[1].get("release", "")).lower() == "revised" else 0,
            -(len(str(item[1].get("source_label_norm") or item[1].get("source_label") or ""))),
        )
    )
    best_score, best_row = scored_rows[0]
    logging.info(
        "Resolved concept %s with source_var=%s label='%s' score=%.2f release=%s",
        concept_key,
        best_row.get("source_var"),
        best_row.get("source_label"),
        best_score,
        best_row.get("release"),
    )
    return best_row, best_score


def determine_prefix(row: Optional[pd.Series], concept: dict) -> Optional[str]:
    if row is None:
        forms = concept.get("forms")
        if forms and len(forms) == 1:
            return forms[0].upper()
        return None
    prefixes = extract_prefixes(row)
    forms = concept.get("forms")
    if forms:
        allowed = [f.upper() for f in forms]
        for form in allowed:
            if form in prefixes:
                return form
        if len(allowed) == 1:
            return allowed[0]
    if prefixes:
        return sorted(prefixes)[0]
    return None


def load_manifest(year: int, root: Path, cache: dict[int, Optional[pd.DataFrame]] = None) -> Optional[pd.DataFrame]:
    if cache is None:
        cache = {}
    if year in cache:
        return cache[year]
    manifest_path = root / str(year) / f"{year}_manifest.csv"
    if not manifest_path.exists():
        logging.warning("Manifest %s missing; will fallback to filesystem scan", manifest_path)
        cache[year] = None
        return None
    manifest = pd.read_csv(manifest_path, dtype={"is_revision": str})
    cache[year] = manifest
    return manifest


def prefer_manifest_row(
    manifest: pd.DataFrame, prefix: str, survey: str, dict_hint: Optional[str] = None
) -> Optional[pd.Series]:
    if manifest is None or manifest.empty:
        return None
    prefix_upper = prefix.upper()
    subset = manifest[manifest["prefix"].astype(str).str.upper() == prefix_upper]
    if subset.empty:
        subset = manifest[manifest["filename"].astype(str).str.contains(prefix_upper, case=False, na=False)]
    if subset.empty:
        return None

    if dict_hint and "dictionary_filename" in subset.columns:
        dict_base = os.path.basename(str(dict_hint))
        with_dict = subset[subset["dictionary_filename"].astype(str) == dict_base]
        if not with_dict.empty:
            subset = with_dict

    subset = subset.copy()
    subset["is_revision_flag"] = subset.get("is_revision", False).astype(str).str.lower().isin(["true", "1", "yes"])
    subset["release_rank"] = subset.get("release", "").astype(str).str.lower().eq("revised").astype(int)
    subset.sort_values([
        "is_revision_flag",
        "release_rank",
    ], ascending=[False, False], inplace=True)
    return subset.iloc[0]


def locate_data_file(
    year: int,
    prefix: Optional[str],
    survey: str,
    root: Path,
    manifest_cache: dict[int, Optional[pd.DataFrame]],
    dict_hint: Optional[str] = None,
) -> tuple[Optional[Path], Optional[str]]:
    if prefix is None:
        logging.warning("Cannot locate data file without prefix for year=%s survey=%s", year, survey)
        return None, None
    manifest = load_manifest(year, root, manifest_cache)
    manifest_row = (
        prefer_manifest_row(manifest, prefix, survey, dict_hint) if manifest is not None else None
    )
    if manifest_row is not None:
        filename = manifest_row.get("filename")
        release = manifest_row.get("release")
        if filename:
            path = root / str(year) / str(filename)
            if path.exists():
                return path, str(release) if release is not None else None
        logging.warning("Manifest pointed to %s but file missing; falling back", filename)
    year_dir = root / str(year)
    if not year_dir.exists():
        logging.error("Year directory %s missing", year_dir)
        return None, None
    candidates = []
    prefix_lower = prefix.lower()
    for ext in (".csv", ".tsv", ".txt", ".parquet", ".xlsx", ".xls"):
        for file in year_dir.glob(f"**/*{prefix}*{ext}"):
            candidates.append(file)
    if not candidates:
        for file in year_dir.glob("**/*"):
            if prefix_lower in file.name.lower() and file.suffix.lower() in {".csv", ".tsv", ".txt", ".xlsx", ".xls", ".parquet"}:
                candidates.append(file)
    if not candidates:
        logging.warning("No files found matching prefix %s in %s", prefix, year_dir)
        return None, None

    dict_tokens: set[str] = set()
    if dict_hint:
        dict_stem = Path(str(dict_hint)).stem
        dict_stem = re.sub(r"(?i)(dict|dictionary)", "", dict_stem)
        dict_tokens = {tok for tok in re.findall(r"[A-Z0-9]+", dict_stem.upper()) if tok}

    def rank(p: Path) -> tuple[int, int, float]:
        name = p.name.upper()
        overlap = sum(1 for t in dict_tokens if t in name) if dict_tokens else 0
        text_pref = 1 if p.suffix.lower() in {".csv", ".tsv", ".txt"} else 0
        return (overlap, text_pref, p.stat().st_mtime)

    candidates.sort(key=rank, reverse=True)
    chosen = candidates[0]
    logging.info("Fallback located %s for prefix %s year %s", chosen, prefix, year)
    return chosen, None


def load_data_file(path: Path, cache: dict[Path, tuple[pd.DataFrame, Optional[str]]]) -> tuple[pd.DataFrame, Optional[str]]:
    if path in cache:
        return cache[path]
    suffix = path.suffix.lower()
    if suffix in {".csv", ".tsv", ".txt"}:
        sep = "," if suffix == ".csv" else "\t"
        df = pd.read_csv(path, dtype=str, sep=sep, na_filter=False, low_memory=False)
    elif suffix == ".parquet":
        df = pd.read_parquet(path)
        df = df.applymap(lambda x: str(x) if not pd.isna(x) else None)
    elif suffix in {".xlsx", ".xls"}:
        df = pd.read_excel(path, dtype=str)
    else:
        raise ValueError(f"Unsupported file type: {suffix}")
    df.columns = [str(col) for col in df.columns]
    unitid_col = None
    for col in df.columns:
        if col.lower() == "unitid":
            unitid_col = col
            break
    if unitid_col is None:
        logging.warning("UNITID column missing in %s", path)
    else:
        df[unitid_col] = pd.to_numeric(df[unitid_col], errors="coerce").astype("Int64")
    cache[path] = (df, unitid_col)
    return cache[path]


def find_source_column(df: pd.DataFrame, source_var: str) -> Optional[str]:
    if source_var in df.columns:
        return source_var
    lookup = {col.lower(): col for col in df.columns}
    return lookup.get(source_var.lower())


def coerce_numeric(series: pd.Series) -> pd.Series:
    if series.dtype.kind in {"i", "u", "f"}:
        return series.astype(float)
    replaced = series.replace(NA_TOKENS, pd.NA)
    return pd.to_numeric(replaced, errors="coerce")


def apply_transform(values: pd.Series, transform: str) -> pd.Series:
    transform = (transform or "identity").strip().lower()
    if transform == "identity":
        return values
    if transform.startswith("scale:"):
        factor = float(transform.split(":", 1)[1])
        return values * factor
    if transform.startswith("divide:"):
        divisor = float(transform.split(":", 1)[1])
        return values / divisor
    if transform == "percent":
        return values * 100.0
    if transform == "negate":
        return -values
    logging.warning("Unknown transform '%s'; applying identity", transform)
    return values


def run_nonnegative_rule(df: pd.DataFrame, rule: dict) -> int:
    vars_set = set(rule.get("target_vars", []))
    subset = df[df["target_var"].isin(vars_set)]
    violations = subset[subset["value"] < 0].shape[0]
    return violations


def run_balance_rule(df: pd.DataFrame, rule: dict) -> int:
    total_var = rule.get("total")
    part_vars = rule.get("parts", [])
    tolerance = float(rule.get("tolerance", 0))
    pivot = df[df["target_var"].isin([total_var] + list(part_vars))]
    if pivot.empty:
        return 0
    pivot = pivot.pivot_table(
        index=["UNITID", "year"],
        columns="target_var",
        values="value",
        aggfunc="first",
    )
    violations = 0
    for _, row in pivot.iterrows():
        total = row.get(total_var)
        parts = row[list(part_vars)].sum(min_count=1)
        if pd.isna(total) or pd.isna(parts):
            continue
        if total + abs(total) * tolerance < parts:
            violations += 1
    return violations


def run_growth_rule(df: pd.DataFrame, rule: dict) -> int:
    target_var = rule.get("target_var")
    max_abs_pct = float(rule.get("max_abs_pct", 0))
    subset = df[df["target_var"] == target_var].dropna(subset=["UNITID", "year", "value"])
    if subset.empty:
        return 0
    subset = subset.sort_values(["UNITID", "year"])
    subset["lag"] = subset.groupby("UNITID")["value"].shift(1)
    subset = subset.dropna(subset=["lag"])
    subset["pct_change"] = (subset["value"] - subset["lag"]) / subset["lag"].replace(0, pd.NA)
    subset = subset.dropna(subset=["pct_change"])
    violations = subset[subset["pct_change"].abs() > max_abs_pct].shape[0]
    return violations


def run_release_policy(df: pd.DataFrame, rule: dict, strict: bool) -> tuple[int, List[str]]:
    allow_mixed = bool(rule.get("allow_mixed_release", True))
    if allow_mixed:
        return 0, []
    violations = 0
    errors: List[str] = []
    grouped = df.groupby(["target_var", "UNITID"])
    for (target_var, unitid), group in grouped:
        releases = set(group["release"].dropna().str.lower())
        if len(releases) > 1:
            violations += 1
            if strict:
                errors.append(
                    f"Mixed release statuses {releases} for target_var={target_var} UNITID={unitid}"
                )
    return violations, errors


def run_validations(df: pd.DataFrame, rules: dict, strict_release: bool) -> tuple[pd.DataFrame, List[str]]:
    records = []
    errors: List[str] = []
    nonneg_rules = rules.get("nonnegatives", []) or []
    for rule in nonneg_rules:
        violations = run_nonnegative_rule(df, rule)
        records.append(
            {
                "rule_type": "nonnegative",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    balance_rules = rules.get("balances", []) or []
    for rule in balance_rules:
        violations = run_balance_rule(df, rule)
        records.append(
            {
                "rule_type": "balance",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    growth_rules = rules.get("growth_caps", []) or []
    for rule in growth_rules:
        violations = run_growth_rule(df, rule)
        records.append(
            {
                "rule_type": "growth_cap",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    release_rule = rules.get("release_policy", {}) or {}
    violations, release_errors = run_release_policy(df, release_rule, strict_release)
    records.append(
        {
            "rule_type": "release_policy",
            "description": release_rule.get("description"),
            "violations": violations,
        }
    )
    errors.extend(release_errors)
    report = pd.DataFrame(records)
    return report, errors


def build_output_frame(records: List[pd.DataFrame]) -> pd.DataFrame:
    if not records:
        return pd.DataFrame(columns=OUTPUT_COLUMNS)
    df = pd.concat(records, ignore_index=True)
    df = df[OUTPUT_COLUMNS]
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    return df


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    configure_logging(args.log_level)
    years = parse_years(args.years)
    lake = load_dictionary_lake(args.lake)
    rules = load_validation_rules(args.rules)

    lake_by_year: dict[int, pd.DataFrame] = {year: lake[lake["year"] == year].copy() for year in sorted(lake["year"].unique())}
    manifest_cache: dict[int, Optional[pd.DataFrame]] = {}
    data_cache: dict[Path, tuple[pd.DataFrame, Optional[str]]] = {}
    decision_records: List[CandidateSelection] = []
    output_frames: List[pd.DataFrame] = []

    for year in years:
        year_lake = lake_by_year.get(year, pd.DataFrame(columns=lake.columns))
        logging.info("Processing year %s with %d dictionary rows", year, len(year_lake))
        for concept_key, concept in CONCEPTS.items():
            candidates_df = filter_candidates_by_forms(year_lake, concept.get("forms"))
            n_candidates = len(candidates_df)
            best_row, best_score = choose_candidate(candidates_df, concept_key, concept) if n_candidates else (None, float("nan"))
            if best_row is None or (pd.notna(best_score) and best_score < MIN_ACCEPT_SCORE):
                logging.warning(
                    "Weak or no match for %s in %s (score=%.2f; candidates=%s)",
                    concept_key,
                    year,
                    float(best_score) if pd.notna(best_score) else float("nan"),
                    n_candidates,
                )
                decision_records.append(
                    CandidateSelection(
                        concept_key=concept_key,
                        year=year,
                        target_var=str(concept.get("target_var")),
                        source_var=None,
                        dict_file=None,
                        score=None if pd.isna(best_score) else float(best_score),
                        prefix=None,
                        release=None,
                        n_candidates=n_candidates,
                        label_matched=None,
                    )
                )
                continue
            prefix = determine_prefix(best_row, concept)
            release = str(best_row.get("release")) if best_row.get("release") is not None else None
            decision_records.append(
                CandidateSelection(
                    concept_key=concept_key,
                    year=year,
                    target_var=str(concept.get("target_var")),
                    source_var=str(best_row.get("source_var")),
                    dict_file=str(best_row.get("dict_file")) if best_row.get("dict_file") is not None else None,
                    score=None if pd.isna(best_score) else float(best_score),
                    prefix=prefix,
                    release=release,
                    n_candidates=n_candidates,
                    label_matched=str(best_row.get("source_label")) if best_row.get("source_label") is not None else None,
                )
            )
            data_path, manifest_release = locate_data_file(
                year,
                prefix,
                str(concept.get("survey", "")),
                args.root,
                manifest_cache,
                dict_hint=str(best_row.get("dict_file")) if best_row.get("dict_file") is not None else None,
            )
            effective_release = manifest_release or release
            if data_path is None:
                logging.warning("No data file found for concept %s year %s prefix %s", concept_key, year, prefix)
                continue
            df, unitid_col = load_data_file(data_path, data_cache)
            if unitid_col is None:
                logging.warning("Skipping concept %s year %s due to missing UNITID in %s", concept_key, year, data_path)
                continue
            source_var = str(best_row.get("source_var"))
            source_col = find_source_column(df, source_var)
            if source_col is None:
                logging.warning("Source var %s not present in %s for concept %s year %s", source_var, data_path, concept_key, year)
                continue
            values = coerce_numeric(df[source_col])
            values = apply_transform(values, str(concept.get("transform", "identity")))
            frame = pd.DataFrame(
                {
                    "UNITID": df[unitid_col],
                    "year": year,
                    "target_var": concept.get("target_var"),
                    "value": values,
                    "concept": concept.get("concept"),
                    "units": concept.get("units"),
                    "survey": concept.get("survey"),
                    "form_family": prefix,
                    "period_type": concept.get("period_type"),
                    "source_var": source_col,
                    "label_matched": str(best_row.get("source_label")) if best_row.get("source_label") is not None else None,
                    "source_file": str(data_path),
                    "release": effective_release,
                    "notes": concept.get("notes"),
                }
            )
            output_frames.append(frame)

    output_df = build_output_frame(output_frames)
    report_df, errors = run_validations(output_df, rules, args.strict_release)

    logging.info("Writing output parquet to %s", args.output)
    output_df.to_parquet(args.output, index=False, compression="snappy")
    logging.info("Writing label audit to label_matches.csv")
    pd.DataFrame([sel.__dict__ for sel in decision_records]).to_csv("label_matches.csv", index=False)
    logging.info("Writing validation report to validation_report.csv")
    report_df.to_csv("validation_report.csv", index=False)

    if errors:
        for err in errors:
            logging.error(err)
        if args.strict_release:
            logging.error("Strict release policy violated; exiting with failure")
            return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())
