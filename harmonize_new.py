"""Label-driven IPEDS harmonizer.

This module implements the replacement for ``harmonize.py`` using a "Power User"
architecture. It consumes the concept catalog defined in ``concept_catalog.py``,
the dictionary lake produced by ``01_ingest_dictionaries.py``, and the manifest
files generated by ``download_ipeds.py``. Concepts are resolved by matching
normalized dictionary labels, data files are located through the per-year
manifest (with fallbacks), and the resulting long-form panel is validated using
``validation_rules.yaml``.

Starting in the 2024-25 collection, Cost of Attendance (COA) and net price
labels may surface under CST forms rather than SFA; the harmonizer honors that
shift through the concept catalog's form mappings.

Example
-------
To harmonize 2017-2018 data after running the downloader and dictionary ingest::

    python harmonize_new.py --root /path/to/raw --lake dictionary_lake.parquet \
        --years 2017:2018 --output panel_long.parquet --strict-release

The script will emit ``panel_long.parquet`` with tidy concept values,
``label_matches.csv`` with the label resolution audit, and
``validation_report.csv`` summarizing post-extraction checks.
"""

from __future__ import annotations

import argparse
import logging
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence

try:  # pylint: disable=wrong-import-position
    import pandas as pd
except ImportError as exc:  # pragma: no cover - startup guard
    sys.stderr.write(
        "pandas/openpyxl/xlrd missing. Run: source .venv/bin/activate && pip install -r requirements.txt\n"
    )
    raise
import yaml

from concept_catalog import CONCEPTS, GLOBAL_EXCLUDE

MIN_ACCEPT_SCORE = 3.5
if MIN_ACCEPT_SCORE < 3.5:  # pragma: no cover - sanity guard
    raise ValueError("MIN_ACCEPT_SCORE must be at least 3.5")
TOPK_AUDIT = 5
FINANCE_STOCK_EXCLUDE = re.compile(r"endowment market value|net assets", re.IGNORECASE)
IMPUTE_FLAG_PATTERN = re.compile(r"(imputation|impute|imputed|status\s*flag)", re.IGNORECASE)

METADATA_NEGATIVES = re.compile(
    r"^(unitid(_p)?|unit id|instnm|institution name|year|state|zip|fips|sector)$",
    re.IGNORECASE,
)

NA_TOKENS = [
    "PrivacySuppressed",
    "NULL",
    "NaN",
    "N/A",
    "D",
    "DP",
    "—",
    ".",
]

OUTPUT_COLUMNS = [
    "UNITID",
    "year",
    "target_var",
    "value",
    "concept",
    "units",
    "survey",
    "form_family",
    "period_type",
    "source_var",
    "label_matched",
    "imputed_flag",
    "source_file",
    "release",
    "notes",
]


def _to_lower(text: Optional[str]) -> str:
    return (text or "").strip().lower()


def report_duplicate_modules() -> None:
    repo_root = Path(__file__).resolve().parent
    targets = {
        "01_ingest_dictionaries.py": repo_root / "01_ingest_dictionaries.py",
        "harmonize_new.py": Path(__file__).resolve(),
        "concept_catalog.py": repo_root / "concept_catalog.py",
    }
    for name, canonical in targets.items():
        canonical_path = canonical.resolve()
        matches = [p.resolve() for p in repo_root.rglob(name)]
        duplicates = [
            p
            for p in matches
            if p != canonical_path and ".venv" not in p.parts and "__pycache__" not in p.parts
        ]
        for dup in duplicates:
            print(f"REMOVE_AFTER_REVIEW duplicate module found: {dup}")


@dataclass
class CandidateSelection:
    concept_key: str
    year: int
    target_var: str
    score: Optional[float]
    n_candidates: int
    source_var: Optional[str]
    label_matched: Optional[str]
    dict_file: Optional[str]
    dict_filename: Optional[str]
    prefix_hint: Optional[str]
    survey_hint: Optional[str]
    chosen_data_file: Optional[str]
    manifest_release: Optional[str]
    notes: Optional[str]
    top_alternates: Optional[str]
    imputed_flag: Optional[str]


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Label-driven IPEDS harmonizer")
    parser.add_argument("--root", type=Path, default=Path("data/raw"), help="Raw data root containing year folders")
    parser.add_argument("--lake", type=Path, default=Path("dictionary_lake.parquet"), help="Dictionary lake parquet path")
    parser.add_argument("--output", type=Path, default=Path("panel_long.parquet"), help="Output parquet path")
    parser.add_argument(
        "--years",
        type=str,
        default="2004:2024",
        help="Year expression: 'YYYY', 'YYYY:YYYY', or comma list 'YYYY,YYYY'",
    )
    parser.add_argument("--rules", type=Path, default=Path("validation_rules.yaml"), help="Validation rules YAML path")
    parser.add_argument("--strict-release", action="store_true", help="Error if mixed provisional/revised releases")
    parser.add_argument("--log-level", type=str, default="INFO", help="Logging level (e.g., INFO, DEBUG)")
    return parser.parse_args(argv)


def configure_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(message)s",
    )


def parse_years(expr: str) -> List[int]:
    expr = expr.strip()
    if ":" in expr and "," in expr:
        raise ValueError("Cannot mix range and comma syntax in --years")
    if ":" in expr:
        start_str, end_str = expr.split(":", 1)
        start, end = int(start_str), int(end_str)
        if end < start:
            start, end = end, start
        return list(range(start, end + 1))
    if "," in expr:
        return [int(part.strip()) for part in expr.split(",") if part.strip()]
    return [int(expr)]


def load_dictionary_lake(path: Path) -> pd.DataFrame:
    logging.info("Loading dictionary lake from %s", path)
    try:
        lake = pd.read_parquet(path)
    except (ImportError, ValueError) as exc:
        message = str(exc).lower()
        if "pyarrow" in message or "fastparquet" in message:
            raise ImportError(
                "pyarrow or fastparquet required for parquet support. Run: source .venv/bin/activate && pip install -r requirements.txt"
            ) from exc
        raise
    if "year" not in lake.columns:
        raise KeyError("dictionary lake must include a 'year' column")
    lake["year"] = lake["year"].astype(int)
    for col in ["source_label_norm", "source_label", "prefix_hint", "survey_hint", "release", "dict_file", "filename"]:
        if col in lake.columns:
            lake[col] = lake[col].astype(str)
    return lake


def load_validation_rules(path: Path) -> dict:
    logging.info("Loading validation rules from %s", path)
    with path.open("r", encoding="utf-8") as fh:
        return yaml.safe_load(fh) or {}


def extract_prefixes(row: pd.Series) -> set[str]:
    prefixes: set[str] = set()
    prefix_hint = str(row.get("prefix_hint", "")).strip()
    if prefix_hint and prefix_hint not in {"nan", "None", ""}:
        prefixes.add(prefix_hint.upper())
    filename = str(row.get("filename", ""))
    dict_file = str(row.get("dict_file", ""))
    pattern = re.compile(r"\b(F1A|F2A|F3A|EF|E12|E1D|EFFY|SFA|CST|OM|GRS|PE)\b", re.IGNORECASE)
    for text in (filename, dict_file):
        for match in pattern.findall(text):
            prefixes.add(match.upper())
    return prefixes


def _parse_income_band(text: str) -> tuple[Optional[int], Optional[int]]:
    cleaned = _to_lower(text)
    cleaned = re.sub(r"[,$\s]+", " ", cleaned)
    match = re.search(r"(?:less than|under)\s*(\d{2,3} ?0{3})", cleaned)
    if match:
        hi = int(match.group(1).replace(" ", ""))
        return 0, hi
    match = re.search(r"(\d{2,3} ?0{3})\s*(?:to|-|–)\s*(\d{2,3} ?0{3})", cleaned)
    if match:
        lo = int(match.group(1).replace(" ", ""))
        hi = int(match.group(2).replace(" ", ""))
        return lo, hi
    match = re.search(r"(\d{2,3} ?0{3})\s*(?:\+|or more|and above)", cleaned)
    if match:
        lo = int(match.group(1).replace(" ", ""))
        return lo, None
    return None, None


def _bands_overlap(a: tuple[Optional[int], Optional[int]], b: tuple[Optional[int], Optional[int]]) -> bool:
    lo_a, hi_a = a
    lo_b, hi_b = b
    if lo_a is None and lo_b is None:
        return True
    lo_a = lo_a or 0
    lo_b = lo_b or 0
    hi_a = hi_a if hi_a is not None else float("inf")
    hi_b = hi_b if hi_b is not None else float("inf")
    return max(lo_a, lo_b) <= min(hi_a, hi_b)


def filter_candidates_by_forms(df: pd.DataFrame, forms: Optional[Sequence[str]]) -> pd.DataFrame:
    if not forms:
        return df.copy()
    allowed = {f.upper() for f in forms}
    mask = []
    for _, row in df.iterrows():
        prefixes = extract_prefixes(row)
        mask.append(bool(prefixes & allowed))
    return df.loc[mask].copy()


def score_candidate(row: pd.Series, concept: dict) -> float:
    label_norm_raw = str(row.get("source_label_norm") or row.get("source_label") or "").strip()
    label_norm_stripped = re.sub(r"[,;$%]", "", label_norm_raw)
    label_norm = label_norm_stripped.lower()

    if METADATA_NEGATIVES.match(label_norm_stripped):
        return -5.0

    score = 0.0
    matched = False
    for pattern in concept.get("label_regex", []):
        regex = re.compile(pattern, re.IGNORECASE)
        if regex.fullmatch(label_norm) or regex.fullmatch(label_norm_stripped) or regex.fullmatch(label_norm_raw):
            score = max(score, 4.0)
            matched = True
            break
        if (
            regex.search(label_norm)
            or regex.search(label_norm_stripped)
            or regex.search(label_norm_raw)
        ):
            score = max(score, 2.0)
            matched = True
    if not matched:
        score -= 1.0
    bonus = 0.0
    forms = concept.get("forms")
    prefixes = extract_prefixes(row)
    if forms:
        allowed = {f.upper() for f in forms}
        if prefixes & allowed:
            bonus = 1.0
        else:
            survey_hint = str(row.get("survey_hint", "")).strip().lower()
            if survey_hint and survey_hint == str(concept.get("survey", "")).strip().lower():
                bonus = 1.0
    else:
        survey_hint = str(row.get("survey_hint", "")).strip().lower()
        if survey_hint and survey_hint == str(concept.get("survey", "")).strip().lower():
            bonus = 1.0
    score += bonus
    units = _to_lower(str(concept.get("units", "")))
    money_hint = bool(re.search(r"\b(dollars?|amount|revenue|expenses?|net price)\b|\$", label_norm))
    count_hint = bool(re.search(r"\bcount|students?|recipients?|headcount|fte\b", label_norm))
    if units in {"usd", "$"}:
        if money_hint:
            score += 0.5
        if count_hint:
            score -= 0.5
    if units in {"count", "fte"}:
        if count_hint:
            score += 0.5
        if money_hint:
            score -= 0.5
    if "band_min" in concept or "band_max" in concept:
        parsed = _parse_income_band(label_norm_raw)
        if parsed != (None, None):
            lo = concept.get("band_min")
            hi = concept.get("band_max")
            if _bands_overlap(parsed, (lo, hi)):
                score += 1.0
            else:
                score -= 1.0
    for pattern in concept.get("exclude_regex", []):
        if re.search(pattern, label_norm, flags=re.IGNORECASE) or re.search(
            pattern, label_norm_raw, flags=re.IGNORECASE
        ):
            score -= 2.0
    return score


def choose_candidate(
    df: pd.DataFrame, concept_key: str, concept: dict
) -> tuple[Optional[pd.Series], float, list[tuple[float, pd.Series]], int]:
    if df.empty:
        return None, float("nan"), [], 0

    filtered_rows: list[pd.Series] = []
    survey_lower = str(concept.get("survey", "")).strip().lower()
    for _, row in df.iterrows():
        source_var = str(row.get("source_var") or "")
        label_norm = str(row.get("source_label_norm") or row.get("source_label") or "")
        if GLOBAL_EXCLUDE.search(source_var) or GLOBAL_EXCLUDE.search(label_norm):
            continue
        if survey_lower == "finance" and FINANCE_STOCK_EXCLUDE.search(label_norm):
            continue
        filtered_rows.append(row)

    if not filtered_rows:
        return None, float("nan"), [], 0

    scored_rows: list[tuple[float, pd.Series]] = []
    for row in filtered_rows:
        score = score_candidate(row, concept)
        scored_rows.append((score, row))

    scored_rows.sort(
        key=lambda item: (
            -item[0],
            -1 if str(item[1].get("release", "")).lower() == "revised" else 0,
            -(len(str(item[1].get("source_label_norm") or item[1].get("source_label") or ""))),
        )
    )

    if not scored_rows:
        return None, float("nan"), [], 0

    top_candidates = scored_rows[:TOPK_AUDIT]
    best_score, best_row = top_candidates[0]

    if pd.isna(best_score) or best_score < MIN_ACCEPT_SCORE:
        return None, best_score, top_candidates, len(filtered_rows)

    logging.info(
        "Resolved concept %s with source_var=%s label='%s' score=%.2f release=%s",
        concept_key,
        best_row.get("source_var"),
        best_row.get("source_label"),
        best_score,
        best_row.get("release"),
    )
    return best_row, best_score, top_candidates, len(filtered_rows)


def coerce_optional_str(value: object) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        cleaned = value.strip()
        if not cleaned or cleaned.lower() in {"nan", "none"}:
            return None
        return cleaned
    if pd.isna(value):
        return None
    text = str(value).strip()
    if not text or text.lower() in {"nan", "none"}:
        return None
    return text


def format_top_alternates(candidates: list[tuple[float, pd.Series]]) -> str:
    parts: list[str] = []
    for score, row in candidates[1:]:
        source_var = coerce_optional_str(row.get("source_var")) or ""
        if not source_var:
            continue
        label = coerce_optional_str(row.get("source_label")) or coerce_optional_str(
            row.get("source_label_norm")
        )
        score_str = "" if pd.isna(score) else f"{score:.2f}"
        parts.append(f"{source_var}|{score_str}|{label or ''}")
    return ";".join(parts)


def determine_prefix(row: Optional[pd.Series], concept: dict) -> Optional[str]:
    if row is None:
        forms = concept.get("forms")
        if forms and len(forms) == 1:
            return forms[0].upper()
        return None
    prefixes = extract_prefixes(row)
    forms = concept.get("forms")
    if forms:
        allowed = [f.upper() for f in forms]
        for form in allowed:
            if form in prefixes:
                return form
        if len(allowed) == 1:
            return allowed[0]
    if prefixes:
        return sorted(prefixes)[0]
    return None


def load_manifest(year: int, root: Path, cache: dict[int, Optional[pd.DataFrame]] = None) -> Optional[pd.DataFrame]:
    if cache is None:
        cache = {}
    if year in cache:
        return cache[year]
    manifest_path = root / str(year) / f"{year}_manifest.csv"
    if not manifest_path.exists():
        logging.warning("Manifest %s missing; will fallback to filesystem scan", manifest_path)
        cache[year] = None
        return None
    manifest = pd.read_csv(manifest_path, dtype={"is_revision": str})
    cache[year] = manifest
    return manifest


def prefer_manifest_row(
    manifest: pd.DataFrame, prefix: str, survey: str, dict_hint: Optional[str] = None
) -> Optional[pd.Series]:
    if manifest is None or manifest.empty:
        return None
    prefix_upper = prefix.upper()
    subset = manifest[manifest["prefix"].astype(str).str.upper() == prefix_upper]
    if subset.empty:
        subset = manifest[manifest["filename"].astype(str).str.contains(prefix_upper, case=False, na=False)]
    if subset.empty:
        return None

    if dict_hint and "dictionary_filename" in subset.columns:
        dict_base = os.path.basename(str(dict_hint))
        with_dict = subset[subset["dictionary_filename"].astype(str) == dict_base]
        if not with_dict.empty:
            subset = with_dict

    subset = subset.copy()
    if "is_revision" in subset.columns:
        subset["is_revision_flag"] = subset["is_revision"].astype(str).str.lower().isin(["true", "1", "yes"])
    else:
        subset["is_revision_flag"] = False
    if "release" in subset.columns:
        subset["release_rank"] = subset["release"].astype(str).str.lower().eq("revised").astype(int)
    else:
        subset["release_rank"] = 0
    subset.sort_values([
        "is_revision_flag",
        "release_rank",
    ], ascending=[False, False], inplace=True)
    return subset.iloc[0]


def locate_data_file(
    year: int,
    prefix: Optional[str],
    survey: str,
    root: Path,
    manifest_cache: dict[int, Optional[pd.DataFrame]],
    dict_hint: Optional[str] = None,
) -> tuple[Optional[Path], Optional[str]]:
    if prefix is None:
        logging.warning("Cannot locate data file without prefix for year=%s survey=%s", year, survey)
        return None, None
    manifest = load_manifest(year, root, manifest_cache)
    manifest_row = (
        prefer_manifest_row(manifest, prefix, survey, dict_hint) if manifest is not None else None
    )
    if manifest_row is not None:
        filename = manifest_row.get("filename")
        release = manifest_row.get("release")
        if filename:
            path = root / str(year) / str(filename)
            if path.exists():
                return path, str(release) if release is not None else None
        logging.warning("Manifest pointed to %s but file missing; falling back", filename)
    year_dir = root / str(year)
    if not year_dir.exists():
        logging.error("Year directory %s missing", year_dir)
        return None, None
    candidates = []
    prefix_lower = prefix.lower()
    for ext in (".csv", ".tsv", ".txt", ".parquet", ".xlsx", ".xls"):
        for file in year_dir.glob(f"**/*{prefix}*{ext}"):
            candidates.append(file)
    if not candidates:
        for file in year_dir.glob("**/*"):
            if prefix_lower in file.name.lower() and file.suffix.lower() in {".csv", ".tsv", ".txt", ".xlsx", ".xls", ".parquet"}:
                candidates.append(file)
    if not candidates:
        logging.warning("No files found matching prefix %s in %s", prefix, year_dir)
        return None, None

    dict_tokens: set[str] = set()
    if dict_hint:
        dict_stem = Path(str(dict_hint)).stem
        dict_stem = re.sub(r"(?i)(dict|dictionary)", "", dict_stem)
        dict_tokens = {tok for tok in re.findall(r"[A-Z0-9]+", dict_stem.upper()) if tok}

    def rank(p: Path) -> tuple[int, int, float]:
        name = p.name.upper()
        overlap = sum(1 for t in dict_tokens if t in name) if dict_tokens else 0
        text_pref = 1 if p.suffix.lower() in {".csv", ".tsv", ".txt"} else 0
        return (overlap, text_pref, p.stat().st_mtime)

    candidates.sort(key=rank, reverse=True)
    chosen = candidates[0]
    logging.info("Fallback located %s for prefix %s year %s", chosen, prefix, year)
    return chosen, None


def load_data_file(path: Path, cache: dict[Path, tuple[pd.DataFrame, Optional[str]]]) -> tuple[pd.DataFrame, Optional[str]]:
    if path in cache:
        return cache[path]
    suffix = path.suffix.lower()
    if suffix in {".csv", ".tsv", ".txt"}:
        sep = "," if suffix == ".csv" else "\t"
        df = pd.read_csv(path, dtype=str, sep=sep, na_filter=False, low_memory=False)
    elif suffix == ".parquet":
        try:
            df = pd.read_parquet(path)
        except (ImportError, ValueError) as exc:
            message = str(exc).lower()
            if "pyarrow" in message or "fastparquet" in message:
                raise ImportError(
                    "pyarrow or fastparquet required for parquet support. Run: source .venv/bin/activate && pip install -r requirements.txt"
                ) from exc
            raise
        df = df.applymap(lambda x: str(x) if not pd.isna(x) else None)
    elif suffix in {".xlsx", ".xls"}:
        engine = "openpyxl" if suffix == ".xlsx" else "xlrd"
        df = pd.read_excel(path, dtype=str, engine=engine)
    else:
        raise ValueError(f"Unsupported file type: {suffix}")
    df.columns = [str(col) for col in df.columns]
    unitid_col = None
    for col in df.columns:
        if col.lower() == "unitid":
            unitid_col = col
            break
    if unitid_col is None:
        logging.warning("UNITID column missing in %s", path)
    else:
        df[unitid_col] = pd.to_numeric(df[unitid_col], errors="coerce").astype("Int64")
    cache[path] = (df, unitid_col)
    return cache[path]


def find_source_column(df: pd.DataFrame, source_var: str) -> Optional[str]:
    if source_var in df.columns:
        return source_var
    lookup = {col.lower(): col for col in df.columns}
    return lookup.get(source_var.lower())


def coerce_numeric(series: pd.Series) -> pd.Series:
    if series.dtype.kind in {"i", "u", "f"}:
        return series.astype(float)
    replaced = series.replace(NA_TOKENS, pd.NA)
    return pd.to_numeric(replaced, errors="coerce")


def apply_transform(values: pd.Series, transform: str) -> pd.Series:
    transform = (transform or "identity").strip().lower()
    if transform == "identity":
        return values
    if transform.startswith("scale:"):
        factor = float(transform.split(":", 1)[1])
        return values * factor
    if transform.startswith("divide:"):
        divisor = float(transform.split(":", 1)[1])
        return values / divisor
    if transform == "percent":
        return values * 100.0
    if transform == "negate":
        return -values
    logging.warning("Unknown transform '%s'; applying identity", transform)
    return values


def run_nonnegative_rule(df: pd.DataFrame, rule: dict) -> int:
    vars_set = set(rule.get("target_vars", []))
    subset = df[df["target_var"].isin(vars_set)]
    violations = subset[subset["value"] < 0].shape[0]
    return violations


def run_balance_rule(df: pd.DataFrame, rule: dict) -> int:
    total_var = rule.get("total")
    part_vars = rule.get("parts", [])
    tolerance = float(rule.get("tolerance", 0))
    pivot = df[df["target_var"].isin([total_var] + list(part_vars))]
    if pivot.empty:
        return 0
    pivot = pivot.pivot_table(
        index=["UNITID", "year"],
        columns="target_var",
        values="value",
        aggfunc="first",
    )
    violations = 0
    for _, row in pivot.iterrows():
        total = row.get(total_var)
        parts = row[list(part_vars)].sum(min_count=1)
        if pd.isna(total) or pd.isna(parts):
            continue
        if total + abs(total) * tolerance < parts:
            violations += 1
    return violations


def run_growth_rule(df: pd.DataFrame, rule: dict) -> int:
    target_var = rule.get("target_var")
    max_abs_pct = float(rule.get("max_abs_pct", 0))
    subset = df[df["target_var"] == target_var].dropna(subset=["UNITID", "year", "value"])
    if subset.empty:
        return 0
    subset = subset.sort_values(["UNITID", "year"])
    subset["lag"] = subset.groupby("UNITID")["value"].shift(1)
    subset = subset.dropna(subset=["lag"])
    subset["pct_change"] = (subset["value"] - subset["lag"]) / subset["lag"].replace(0, pd.NA)
    subset = subset.dropna(subset=["pct_change"])
    violations = subset[subset["pct_change"].abs() > max_abs_pct].shape[0]
    return violations


def run_release_policy(df: pd.DataFrame, rule: dict, strict: bool) -> tuple[int, List[str]]:
    allow_mixed = bool(rule.get("allow_mixed_release", True))
    if allow_mixed:
        return 0, []
    violations = 0
    errors: List[str] = []
    grouped = df.groupby(["target_var", "UNITID"])
    for (target_var, unitid), group in grouped:
        releases = set(group["release"].dropna().str.lower())
        if len(releases) > 1:
            violations += 1
            if strict:
                errors.append(
                    f"Mixed release statuses {releases} for target_var={target_var} UNITID={unitid}"
                )
    return violations, errors


def run_validations(df: pd.DataFrame, rules: dict, strict_release: bool) -> tuple[pd.DataFrame, List[str]]:
    records = []
    errors: List[str] = []
    nonneg_rules = rules.get("nonnegatives", []) or []
    for rule in nonneg_rules:
        violations = run_nonnegative_rule(df, rule)
        records.append(
            {
                "rule_type": "nonnegative",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    balance_rules = rules.get("balances", []) or []
    for rule in balance_rules:
        violations = run_balance_rule(df, rule)
        records.append(
            {
                "rule_type": "balance",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    growth_rules = rules.get("growth_caps", []) or []
    for rule in growth_rules:
        violations = run_growth_rule(df, rule)
        records.append(
            {
                "rule_type": "growth_cap",
                "description": rule.get("description"),
                "violations": violations,
            }
        )
    release_rule = rules.get("release_policy", {}) or {}
    violations, release_errors = run_release_policy(df, release_rule, strict_release)
    records.append(
        {
            "rule_type": "release_policy",
            "description": release_rule.get("description"),
            "violations": violations,
        }
    )
    errors.extend(release_errors)
    report = pd.DataFrame(records)
    return report, errors


def build_output_frame(records: List[pd.DataFrame]) -> pd.DataFrame:
    if not records:
        return pd.DataFrame(columns=OUTPUT_COLUMNS)
    df = pd.concat(records, ignore_index=True)
    df = df[OUTPUT_COLUMNS]
    df["value"] = pd.to_numeric(df["value"], errors="coerce")
    return df


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    configure_logging(args.log_level)
    years = parse_years(args.years)
    report_duplicate_modules()
    lake = load_dictionary_lake(args.lake)
    rules = load_validation_rules(args.rules)

    lake_by_year: dict[int, pd.DataFrame] = {year: lake[lake["year"] == year].copy() for year in sorted(lake["year"].unique())}
    manifest_cache: dict[int, Optional[pd.DataFrame]] = {}
    data_cache: dict[Path, tuple[pd.DataFrame, Optional[str]]] = {}
    decision_records: List[CandidateSelection] = []
    output_frames: List[pd.DataFrame] = []

    for year in years:
        year_lake = lake_by_year.get(year, pd.DataFrame(columns=lake.columns))
        logging.info("Processing year %s with %d dictionary rows", year, len(year_lake))
        for concept_key, concept in CONCEPTS.items():
            candidates_df = filter_candidates_by_forms(year_lake, concept.get("forms"))
            raw_candidates = len(candidates_df)
            best_row: Optional[pd.Series]
            best_score: float
            top_candidates: list[tuple[float, pd.Series]]
            candidate_count = 0
            if raw_candidates:
                best_row, best_score, top_candidates, candidate_count = choose_candidate(
                    candidates_df, concept_key, concept
                )
            else:
                best_row, best_score, top_candidates = None, float("nan"), []
            top_alternates = format_top_alternates(top_candidates)
            primary_row = best_row if best_row is not None else (top_candidates[0][1] if top_candidates else None)
            dict_file = coerce_optional_str(primary_row.get("dict_file")) if primary_row is not None else None
            dict_filename = os.path.basename(dict_file) if dict_file else None
            prefix_hint = coerce_optional_str(primary_row.get("prefix_hint")) if primary_row is not None else None
            survey_hint = coerce_optional_str(primary_row.get("survey_hint")) if primary_row is not None else None
            label_matched = coerce_optional_str(best_row.get("source_label")) if best_row is not None else None
            source_var_val = coerce_optional_str(best_row.get("source_var")) if best_row is not None else None
            score_val = None if pd.isna(best_score) else float(best_score)
            record_kwargs = {
                "concept_key": concept_key,
                "year": year,
                "target_var": str(concept.get("target_var")),
                "score": score_val,
                "n_candidates": candidate_count,
                "source_var": source_var_val,
                "label_matched": label_matched,
                "dict_file": dict_file,
                "dict_filename": dict_filename,
                "prefix_hint": prefix_hint,
                "survey_hint": survey_hint,
                "chosen_data_file": None,
                "manifest_release": None,
                "notes": concept.get("notes"),
                "top_alternates": top_alternates,
                "imputed_flag": None,
            }
            if best_row is None:
                logging.warning(
                    "Weak or no match for %s in %s (score=%.2f; candidates=%s)",
                    concept_key,
                    year,
                    float(best_score) if pd.notna(best_score) else float("nan"),
                    candidate_count,
                )
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            prefix = determine_prefix(best_row, concept)
            release = coerce_optional_str(best_row.get("release"))
            data_path, manifest_release = locate_data_file(
                year,
                prefix,
                str(concept.get("survey", "")),
                args.root,
                manifest_cache,
                dict_hint=dict_file,
            )
            if manifest_release:
                record_kwargs["manifest_release"] = manifest_release
            if data_path is not None:
                record_kwargs["chosen_data_file"] = str(data_path)
            effective_release = manifest_release or release
            if data_path is None:
                logging.warning(
                    "No data file found for concept %s year %s prefix %s", concept_key, year, prefix
                )
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            df, unitid_col = load_data_file(data_path, data_cache)
            if unitid_col is None:
                logging.warning(
                    "Skipping concept %s year %s due to missing UNITID in %s", concept_key, year, data_path
                )
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            source_var = source_var_val or ""
            source_col = find_source_column(df, source_var)
            if source_col is None:
                logging.warning(
                    "Source var %s not present in %s for concept %s year %s",
                    source_var,
                    data_path,
                    concept_key,
                    year,
                )
                decision_records.append(CandidateSelection(**record_kwargs))
                continue
            values = coerce_numeric(df[source_col])
            values = apply_transform(values, str(concept.get("transform", "identity")))
            xvar = f"X{source_col}"
            impute_lookup = {str(col).lower(): col for col in df.columns}
            xvar_match = impute_lookup.get(xvar.lower())
            candidate_imputed_cols = (
                [xvar_match]
                if xvar_match is not None
                else [col for col in df.columns if IMPUTE_FLAG_PATTERN.search(str(col))]
            )
            preferred_imputed = [
                col
                for col in candidate_imputed_cols
                if source_col.lower() in str(col).lower()
            ]
            imputed_col = preferred_imputed[0] if preferred_imputed else (candidate_imputed_cols[0] if candidate_imputed_cols else None)
            if imputed_col is not None:
                record_kwargs["imputed_flag"] = str(imputed_col)
                imputed_values = df[imputed_col]
            else:
                imputed_values = pd.Series(pd.NA, index=df.index)
            frame = pd.DataFrame(
                {
                    "UNITID": df[unitid_col],
                    "year": year,
                    "target_var": concept.get("target_var"),
                    "value": values,
                    "concept": concept.get("concept"),
                    "units": concept.get("units"),
                    "survey": concept.get("survey"),
                    "form_family": prefix,
                    "period_type": concept.get("period_type"),
                    "source_var": source_col,
                    "label_matched": label_matched,
                    "imputed_flag": imputed_values,
                    "source_file": str(data_path),
                    "release": effective_release,
                    "notes": concept.get("notes"),
                }
            )
            output_frames.append(frame)
            decision_records.append(CandidateSelection(**record_kwargs))

    output_df = build_output_frame(output_frames)
    report_df, errors = run_validations(output_df, rules, args.strict_release)

    logging.info("Writing output parquet to %s", args.output)
    output_df.to_parquet(args.output, index=False, compression="snappy")
    logging.info("Writing label audit to label_matches.csv")
    audit_columns = [
        "year",
        "concept_key",
        "target_var",
        "score",
        "n_candidates",
        "source_var",
        "label_matched",
        "dict_file",
        "dict_filename",
        "prefix_hint",
        "survey_hint",
        "chosen_data_file",
        "manifest_release",
        "notes",
        "top_alternates",
        "imputed_flag",
    ]
    audit_df = pd.DataFrame([sel.__dict__ for sel in decision_records])
    if audit_df.empty:
        audit_df = pd.DataFrame(columns=audit_columns)
    else:
        audit_df = audit_df[audit_columns]
    audit_df["accepted"] = audit_df["score"].apply(
        lambda s: s is not None and not pd.isna(s) and float(s) >= MIN_ACCEPT_SCORE
    )
    audit_df.to_csv("label_matches.csv", index=False)
    logging.info("Writing validation report to validation_report.csv")
    report_df.to_csv("validation_report.csv", index=False)

    coverage_fail = False
    if args.strict_coverage:
        weak = audit_df[(audit_df["year"].isin(years)) & (~audit_df["accepted"])]
        if not weak.empty:
            coverage_fail = True
            logging.error(
                "Strict coverage failed for %d rows. Examples:\n%s",
                len(weak),
                weak.head(10).to_string(index=False),
            )

    if errors or coverage_fail:
        for err in errors:
            logging.error(err)
        if args.strict_release:
            logging.error("Strict release policy violated; exiting with failure")
            return 1
        if coverage_fail:
            logging.error("Strict coverage violated; exiting with failure")
            return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())
